---
title: "Tag 8"
date: 2021-12-16
---
_Liebes Tagebuch_,

 
**Finale Übung Datenintegration und der Vergleich VuFind und Solr**

Zuerst musste ich die **Suchmaschine Solr** nochmals starten, es gab eine Fehlermeldung bei auto konfiguration.

/usr/local/vufind/solr.sh start

**Forschungsinformationssysteme**
OAI-PMH : Selektive Abfragen oder keine selektiven Anfragen?
OAI-PMH erlaubt insofern selektive Anfragen, das was man aus der Schnittstelle haben kann ,filtern kann aus Änderungsdaten, da hat man aus DSpace aus langer Liste von Communites und Collection (Set) herausgesucht.Also keine Suchmaschine. Eher keine Selektive Schnittstelle.
Beispiele für selektive Schnittstellen:
•	SRU wäre eine wie Google Suchanfrage kann formulieren und Ausschlusskriterien.
•	Open search war mal von Amazon entwickelt worden.
•	Solr API (Suchmaschine zusammen mit Vufind haben wir die ja installiert).

**XML Dateiheader hat gefehlt, darum DSpace nicht funktioniert**
Tag 6 da haben wir im Marc Edit  aus Koha und DSpace in Dublin Core und dann in Marc XML wollten wir konvertieren, da gab es immer Fehlermeldungen. Das manuelle herauskopieren. Koha funktioniert weil schon in Marc XML, aber DSpace ja nicht. Tag 6 wurde noch ergänzt. 
_Warum hat es nicht funktioniert?
Also diese Webansicht, zeigte nur die inhaltlichen Datenfelder von Dublin Core. Aber der XML Dateiheader hat da gefehlt. Weil der Header enthält ja die Namespaces, der deklariert, wie die XML Elemente deklariert sind._


**Alternative Software zu Open Refine sind***:

**Catmandu** ist in Pearl programmiert, wurde mit Libre Cat programmiert (ähnlich wie DSpace).E hat eine OAI—PMH Schnittstelle zum Datenabfragen, werden dann von Catmandu heruntergeladen. Bibliotgrafische kann von Marc, Mods, Dublin Core konvertieren. Kann auch in RDF konvertieren um dann Linked Data zu machen im Semantischen Kontext einzusetzen. Datensätze, wo ich auf Festplatte habe, kann in einen Suchindex wie SolR, Elastic Search oder MongoDBoder Datenbank importiert werden (das machen wir ja heute mit Vufind und SolR).Kann übliche Fehler mit Catmandu (Parameter fix) dann repariert die Marc Datei, ohne dass man händisch wie in open refine regeln muss.Expertenwerkzeug, weil keine grafische Oberfläche, man sieht die Änderungen nicht!


**Metafature**	Ist für linked Data.Ene Datenprozessierung hat im Hintergrund von Normdatenbanken in ein semantisches Format.Metadata und Manufacture, ist in Java programmiert (ähnlich wie Catmandu von Funktionen her).

**Marc Edit** Das kennen wir schon vom Tag 6.

In Praxis nimmt man meistens das Tool wo man am besten beherrscht oder man muss abwägen, wenn ein anderes Tool gut wäre, wo auch anderer mitwirken, dann sollte man das Tool nehmen, wo Aufgabe weniger komplex ist.

**Nutzung von JSON-APIs für Web API’s**
In XML mit OAI-PMH ist eigentlich schon veraltet, jetzt wären besser JSON-APIs in der Praxis setzen sich solche durch.
Java Script Object Obtation
Jason ist ein Feld, das kann in einem anderen Feld wieder sein.
Bei XML wären Attribute und dann mit spitzen Klammern.
Man kann auch über Browser das aufrufen.
Der Firefox Browser visualisiert das auch gut, kann Felder auf-und zuklappen.

**Lobid** Aus Hochschule Nordrheinwestflaen da wurden aus GND (gemeinsamer Normdateien)
Ich kann aber ein Datensatz nicht maschinell abrufen, da springt lobid
Bekomme Ergebnisse in JSON Format, wenn JSON suche.
Es gibt keine Zeilenumbrüche!
Möchte ich alles haben, dann bulk download, aber jetzt alles in json Format.
Wie bei Google bekommt man Vorschläge, also Vorschlagsliste generieren. Kommunikation findet über JSON ab. Gib mir mal alles was im Suchindex passen könnte.Per Java Script kamen die Vorschläge, GND von diesem Datensatz wurde auch abgerufen also die Nummer.Nun bekommt man genau einen Treffer nämlich der Walter von der Vogelweide.Hätte man nur nach Vogelweide gesucht, hätte man viel mehr Treffer bekommen.
Das ist grosse Stärke von JSON, muss ja schnell gehen die Abfrage in solchen Kontext wird statt XML lieber auf JSON gesetzt.
Da ich keine Nummer habe, kommen 100 Treffer…

Beispiel für Tool: **scraAPlr**
z. b. Google Books wenn man Suchvorschläge machen möchte

 
**AHA-Moment: **Ich könnte sogar jetzt YouTube mit der ScrAPIr so durchsuchen!!**
![Screenshot from 2021-12-16 09-04-18](https://user-images.githubusercontent.com/90834735/146373198-36127fa9-a250-4bdf-9cd4-013542fe4d00.png)

 

Ein anderes Beispiel wurde gezeigt, man kann tatsächlich auch **Google Books mit Scrapir suchen**.
 

**VuFind**
Von Villanova University, ist in PHP programmiert, setzt auf SolR Suchmaschine =Apache.
Aber Funktionaliäten ist Teil von VUFIND, auch die Ernte in VUFINd Harvest wo die OAHI-PMH Schnittstellen abfragen können.
Ist auch alles auf GitHub offengelegt (Script, Code).
Grosse Community auch in Deutschland. Hat immer genug Stakeholder und Contributor, sollte nicht einbrechen das VuFind. 10 Jahre VUFIND! 
Die Universitätsbibliohtek Leipzig, man sieht sehr schnell wenn der Kern VUFInd ist.Die setzen auch VUFIND ein und ein paar andere open source software. 

**Wie geht’s um die Gesundheit von Open Source Community?**.
Kann man auch von Github sich aufrufen. Es sollten immer genug Beteiligte (Contributor) sein, sonst passiert das wie beim Lock 4Jay.
Oder Chaos Metriken gibt’s auch.
 
**Exkurs: Lock 4JAY**
Kritische Zero-Day-Lücke in Log4j gefährdet zahlreiche Server und Apps kam in den Nachrichten überall. Betroffen sind z.B. Apple, Twitter, Steam, Amazon und kleinere Anbieter.Ein kleines Stück Software, bei fast jeder Software wird eingesetzt.
Eine Multimilliondollar-Firma soll zahlen und nicht jammern, wenn Lock for Jay nicht mehr tut, wenn sie Lock for Jay einsetzte. Man sollte die freiwilligen Entwickler bezahlen. Lock for Jay ist auch in VuFind. Isch ein wackliger Bauteil.
Aus vielen kleinen und noch kleineren Bauteilen, an einem kleinen tragenden Teil wird von einer Person aus Nebraska, dann bricht der ganze schöne Teil zusammen. Ist leider sehr nahe an der Realität,wie man das bei Lock for Jay auch gesehen hat. 


**Solr = nur reine Suchmaschine, aber kein Katalog oder Recherchemöglichkeit!**
Ist so quasi Industriestandard das Primo von Ex Libris basiert auf Solr, VuFind basiert auf SolR, SolR ist der Kern.
Alle grossen Webseiten (die nicht gerade von Google kommen) die eine Suche anbieten basieren auf SolR.
Haben wir auf jeden Fall als Anwender schon mal Kontakt haben.
Datumsfeld, alles andere lehnt er ab zb.. Ein Algorithmus kann viel schneller arbeiten, wenn er weiss dass die Daten immer einer ganz bestimmten Form z. b. Zeitintervall, Datum da geht die Suche dann viel schneller. Darum muss man eben strikte Datentypen machen.
 
 
**Daten reinstopfen und abfragen, was ist jetzt der Unterschied zwischen Suchindex (SolR) und einer Datenbank (My SQL)?**
Solr denkt nur in flachen Dokumente, MY SQL denkt in relationale Datensätze.
Man bekommt keine Teile in einem Objekt, bei MYSQL da ist das Objekt in einer Vielzahl von Daten und verweisen aufeinander. Man muss also mehrere Datensätze für Personen, Schlagwörter, immer eigenen Tabellen für Personen, eigene Tabellen für Objekte, man müsste alle Tabellen dann einzeln abfragen. Tabellen eben gut bei relationalen Datenbanken.
Ein Suchindex, der würde den Autor mehrfach ablegen, weil er zu mehreren Objekten gehört und  ein solcher Index ist stark in grammatische Grundform das Wort zurückzuführen, dass kann eine relationale Datenbank nicht. Eine relationale Datenbank nimmt genaue Zeichenfolge so an.
Aber ein Suchindex ist deutlich mächtiger, er nimmt semantisch die Zeichenfolge ab.
Suchindex da findet keine Überprüfung statt, kann mehrere Datensätze reinmachen die gleichen. Bei relationale Datenbank my SQL muss redunant sein, es speichert auch, wenn er abstürzt.
Einmal in den Suchindex reingemachte Daten, lassen sich nicht mehr verändern, man kann nicht löschen. Wenn man Datensatz verändern möchte, dann tut er das löschen und ersetzt durch neuen.
Relationale Datensätze (my sql) tut die alten Datensätze mit den neuen Datensätzen verändern und nicht ersetzen.
Ein Suchindex bietet nur das Abrufen von Datenbanken an, eine my SQL-Datenbank also relationale kann alles machen siehe CRUD : Create, Read, Update, Delete machen.

**Solr**
Backend von Solr
Spannend der Core Selector, mann kann verschiedene Datentopfe so pflegen also authority, biblio, reserves (Vormerkungen), website (seine Bibliothekswebseite über vufind). Core= Kerne
 250 Datensätze sind drin, meine Beispieldatensätze.
Max doch wären 500, wenn ich zweimal laden würde die Beispieldaten.
Segment Count, mit jeder Importierung von Datensätze ist ein Segment. Also eine Datei so quasi. Alle Segmente werden so durchsucht, mit zunehmender Anzahl von Segmenten wird das mühsam. Bei uns sind jetzt 3 Segmente drin.

Kann diesen Suchindex jetzt auch durchsuchen mit Query
Schema gibt eine Unique Key field
Im Dropdown die verschiedenen Datenfelder Autoren…
Stored= diesen Inhalt kann man aus dem index wieder angeben, Stammformen durchsuchen.
![Screenshot from 2021-12-16 09-52-12](https://user-images.githubusercontent.com/90834735/146373641-6f69a399-d4a9-4e69-94b5-45b139f50e05.png)


**Der Vergleich- hinter die Kulissen schauen**:  Vergleich mit Terminal, Vergleich mit Backend von vufind und vufind Benutzeroberfläche
Control c und q=quit konnte ich wieder rausgehen.
Terminal war dieser Befehl:  ist ein Programm das less


**Aufgabe: Suche nach psychology bei Solr Admin, mit Terminal, SolrBenutzer**

hier die Ansicht in **SolrAdmin**:  http://localhost:8983/solr/#/biblio/query
allfields:psychology
![Screenshot from 2021-12-16 10-17-37](https://user-images.githubusercontent.com/90834735/146373957-96a63025-1c71-41f1-8392-2819f9472874.png)

 
**Wo sind Unterschiede?**
 
**SolrAdmin**: aktualisieren von Datensätze kann man über die blaue Versionsnummer, dann ändert sich auch die Nummer, ist so eine interne Verwaltung der Suchmaschine, ist für Suche selbst nicht so relevant. Aber es stimmt.



**SolrBenutzer**: 3 Suchanfragen sofort ersichtlich
**Terminal**: Um die Suchanfrage im Terminal mitzuverfolgen musste folgender Befehl eingegeben werden: 
less +F /usr/local/vufind/solr/vufind/logs/solr.log                   dieser konnte mit Control + C  und dann Q=quit wieder verlassen werden, man kan wieder auf das Start-Terminal zurück.
URL kommt da raus. Einige Anfragen. Man musst fast Adleraugen haben!

Es gibt Hauptanfragen und Hintergrundanfragen.
Es gab Hits =3 , Status 0. 3 Treffer bei allen 3.

 
 **AHA-Moment: Ich kann auch das Ranking/Gewichtung im Terminal sehen!**
Titelfeld wichtiger als Autorenfeld damit hängt es zusammen.
Da haben wir ja nur 3 Treffer, einfaches Ranking.
Ist tatsächlich die Gewichtung / Ranking also wird mit 750 multipliziert, der kurztitel wird also sehr hoch gewichtet, dann kommt der volle Titel mit 600 Titel…
Den Alternativtitel mit 500.
Autorenfeld mit 300.
QF zählt alle Felder auf und gibt pro Feld noch die Gewichtung.
Zahl, aber die werden noch mit 1 gewichtet.
hier der Screenshot dazu:

![Screenshot from 2021-12-16 12-45-43](https://user-images.githubusercontent.com/90834735/146366969-9db35fcd-bb1b-4564-a5b4-21f54a338e94.png)


 
 
**AHA-Moment**: Autoverfollständigung, rechts im Terminal kommt dann die Anfrage dazu gleichzeitig
![Screenshot from 2021-12-16 10-59-17](https://user-images.githubusercontent.com/90834735/146363848-c4267219-ab09-4cba-8f7d-48c84fac8fd3.png)
**: Im Terminal sehe ich bereits meine Suchanfrage, also die Autoverfollständigung der Suche diese ist parallel im Terminal ersichtlich hier der Screenshot!
	

 
**Finale übung: Datenintegration**

**Ziel des Kurses: Import der Marc Edit mit Open Refine konvertierete Daten aus Koha, Archive Space, Dpsace und DOAJ mithilfe von VUFind**

Zuerst mussten wir die Testdaten löschen diese 250 Dateien.Jetzt haben wir wieder einen leeren Index.
Ich habe nicht meine eigenen gesammelten Daten von Koha, ArchiveSpace, doja und Dspace genommen, sondern die empfohlenen Beispieldaten, welche dann als Ordner
Koha, ArchiveSpace, DOJA, Dspace heruntergeladen werden konnten.Die Daten von DSpace in Marc XML, Kohar war schon in Marc XML.
Dann mit dem Befehl **gedit /usr/local/vufind/import/marc_local.properties**   kam man auf die Marc Properties- Config-DAtei. Das war im Editor, dort musste man die Raute  wegnehmen, dann muss koha, dpace, archivespace, doaj jedes mal da einfügen bei der collection.
Danach mit dem Befehl **for f in ~/Downloads/koha/*.xml; do /usr/local/vufind/import-marc.sh $f; done*** konnte ich die gesammelten Beispieldaten aus koha, dpace, archivespace, Doaj raufladen ins VuFind.

**Finale übung Ergebnisse**


![Screenshot from 2021-12-16 12-01-20](https://user-images.githubusercontent.com/90834735/146375636-fba8f775-5bca-4189-893d-3ad3deae7916.png)

Die Institution habe wieder reingenommen bei den Facetten, da bin ich nochmals in die Config-Dateien gegangen.
Nun mein Screenshot hier. Es waren 11 Ergebnisse. **Die Dateien von Koha sind drin und die von Doaj**.


Wir haben alle 11 Resultate bekommen aus den 4 Bib-und Archivsystemen:
- Archive space hat nicht funktioniert
- Koha hat funktioniert
- Doaj hat funktioniert 
- Dspace hat nicht funktioniert

**Warum nicht funktioniert?**

**Erste Lösung**: **In Texteditor also die 001 manuell reinfügen, aber sicher nicht für viele Datensätze.**
 
**Zweite Lösung: Oder wir machen in den marc properties**
Nimm titelfeld und nicht 001, aber es würde ein problem geben dann weil titel ist nicht eindeutiger Identifier. Also müsste man beim Export da was anderes machen, dass das feld 001 mitgeführt wird!
Titel wird mit245 befüllt
Und Untertiel mit 245 a
Vufind hat also Daten zusätzlich angereichert!

**Ursache für Fehler des Imports der ArchivesSpace und DSpace Daten?**
id ist nicht vorhanden (Fehlermeldung - id: null & Document is missing mandatory uniqueKey field: id)
Die Dateien von DSpace und Archive Space hat es nicht genommen, weil eben dieses Feld001 nicht ausgefüllt war.




Tschau liebes Tagebuch!
