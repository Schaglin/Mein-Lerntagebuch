**Tag8** 

Zuerst musste ich die **Suchmaschine Solr** nochmals starten, es gab eine fehlermeldung bei auto konfiguration

/usr/local/vufind/solr.sh start

Forschungsinformationssystemen
OAI-PMH : Selektive Abfragen oder keine selektiven Anfragen?
OAI-PMH erlaubt insofern selektive Anfragen, das was man aus der Schnittstelle haben kann ,f iltern kann aus Änderungsdaten, da hat man aus DSpace aus langer Liste von Communites und Collection (Set) herausgesucht.
Hätte auch alle Datensätze im Zeitraum von… abfragen können?
Aber ich kann nicht mit Titelfeld…xy abgeben kann.
Also keine Suchmaschine. Eher keine Selektrive Schnittstelle.
Beispiele für selektive Schnittstellen:
•	SRU wäre eine wie Google Suchanfrage kann formulieren und Ausschlusskriterien.
•	Open search war mal von Amazon entwickelt worden
•	Solr API (Suchmaschine zusammen mit Vufind haben wir die ja installiert)

Metadaten modellieren und Schnittstellen nutzen
Tag 6 da haben wir im Marc Edit  aus Koha und DSpace in Dublin Core und dann in Marc XML wollten wir konvertieren, da gab es immer Fehlermeldungen. Das manuelle herauskopieren. Koha funktioniert weil schon in Marc XML, aber DSpace ja nicht. Tag 6 wurde noch ergänzt.
Also diese Webansicht, zeigte nur die inhaltlichen Datenfelder von Dublin Core. Aber der XML Dateiheader hat da gefehlt. Weil der Header enthält ja die Namespaces, der deklariert wie die XML Elemente deklariert sind.
Gibt Arbeit nur Metadaten- Management


**Vergleich mit anderen Tools**
**Open Refine**
•	Jeder Transformation von Open REfine sehe ich Ergebnis, merke was eigetlich passiert.
•	Über Historie kann vorwärts und rückwaärts
•	Eigene Scriptsprache =Grell, könönte auch in Jython und Clojure machen
•	Regeln in strukturierter Form formulieren, kein Buttonklicken
•	Reconilicaiton aus Wikidata haben wir ja noch abgerufen auf Baisis der ISBN, haben wir ja die Titel der Zeitschriften abgerufen.

*Alternative Software zu Open Refine sind:

**Catmandu**
•	Catmandu in Pearl programmiert, wurde mit Libre Cat programmiert (ähnlich wie DSpace)
•	Features: OAI—PMH Schnittstelle zum Datenabfragen, werden dann von Catmandu heruntergeladen. Bibliotgrafische kann von Marc, Mods, Dublin Core convertieren
•	Kann auch in RDF konvertieren um dann Linked Data zu machen im Semantischen Kontext einzusetzen
•	Datensätze wo ich auf Festplatte habe, kann in einen Suchindex wie SolR, Elastic Search oder MongoDBoder Datenbank importiert werden (das machen wir ja heute mit Vufind und SolR)
•	Kann übliche Fehler mit Catmandu (Parameter fix) kdann repariert die Marc Datei ohne dass man händisch wie in open refine regeln muss
•	Expertenwerkzeug, weil keine grafische oberfläche. Es zeigt keine Änderungen grafisch an.


**Metafature**
•	Ist für linked Data
•	Eine Datenprozessierung hat im Hintergrund von Normdatenbanken in ein semantisches Format.
•	Metadata und Manufacture
•	In Java programmiert
•	Ähnliche Funktionen wie Catmandu
•	Kann auch als Programmbibliothek in Java verwendet werden, als Werzeug interne Datentransformationen durchzuführen, weil beinhaltet Programmierschnittstellen kann innerhalb Software aufgerufen werden

**Marc Edit** Das kennen wir schon vom Tag 6.

In Praxis nimmt man meistens das Tool wo man am besten beherrscht oder man muss abwägen, wenn ein anderes Tool gut wäre, wo auch anderer mitwirken, dann sollte man das Tool nehmen, wo Aufgabe weniger komplex ist.

**Nutzung von JSON-APIs für Web API’s**
In XML mit OAI-PMH ist eigentlich schon veraltet, jetzt wären besser JSON-APIs in der Praxis setzen sich solche durch
Java Script Object Obtation
Jason ist ein Feld, das kann in einem anderen Feld wieder sein
Bei XML wären Attribute und dann mit spitzen Klammern
Man kann auch über Browser das aufrufen
Der Firefox Browser visualisiert das auch gut, kann Felder auf und zuklappen

**Lobid** Aus Hochschule Nordrheinwestflaen da wurden aus GND (gemeinsamer Normdateien)
Ich kann aber ein Datensatz nicht maschinell abrufen, da springt lobid
Bekomme Ergebnisse in JSON Format wenn JSON suche
Es gibt keine Zeilenumbrüche!
 
Möchte alles haben, dann bulk download, aber jetzt alles in json Format
 
Ich kodiere meine Datensätze in JSon, ich verpflichte mich lediglich ,weil Zeilenumbrüche aber mache, da verpflichte ich mich bei JSon Lines
 
Wie bei Google bekommt man Vorschläge, also Vorschlagsliste generieren. Kommunikation findet über JSON ab. Gib mir mal alles was im Suchindex passen könnte.	Per JAVA Script kamen die Vorschläge, GND von diesem Datensatz wurde auch abgerufen also die Nummer.Ich bekomme jetzt genau einen Treffer nämlich der Walter von der Vogelweide
 
Hätte man nur nach Vogelweide gesucht, hätte ich viel mehr Treffer bekommen
Das ist grosse Stärke von JSON, muss ja schnell gehen die Abfrage in solchen Kontext wird statt XML lieber auf JSON gesetzt.
Da ich keine Nummer habe, kommen 100 Treffer…

Beispiel für Tool: scraAPlr

z. b. Google Books wenn man Suchvorschläge machen möchte

 


**AHA-Moment: **Ich könnte sogar jetzt YouTube mit der ScrAPIr so durchsuchen!!**
![Screenshot from 2021-12-16 09-04-18](https://user-images.githubusercontent.com/90834735/146373198-36127fa9-a250-4bdf-9cd4-013542fe4d00.png)

 
Das wäre in JSON die Struktur
 
Das wäre grafische Darstellungsform
 
 
Ein anderes Beispiel wurde gezeigt, man kann tatsächlich auch **Google Books mit ScrAPIr suchen**.
Das wäre in open refine? Da ist google books
 Kann Author rausnehmen..Komme direkt auf das Buch
 
 

**VuFind**
Von Villanova University
Ist in PHP programmiert, setzt auf SolR SUchmaschine, Apapche
Aber FUnktionaliäten ist Teil von VUFIND wia auch die Ernte in VUFINd Harvest wo die OAHI-PMH Schnittstellen abfragen können
Ist auch alles auf GitHub offengelegt
Grosse Community auch in Deutschland
Hat immer genug Stakeholder, sollte nicht einbrechen das VuFind
10 Jahre VUFIND! Wie geht’s um die Gesundheit von Open Source Community
Kann man auch von Github sich aufrufen
Oder CHaoss Metriken gibt’s auch
 


**Exkurs: Lock 4JAY**
Kritische Zero-Day-Lücke in Log4j gefährdet zahlreiche Server und Apps | heise online
Über eine kritische Zero-Day-Sicherheitslücke namens Log4Shell in der weitverbreiteten Java-Logging-Bibliothek Log4j können Angreifer beliebigen Code ausführen lassen. Betroffen sind etwa Dienste von Apple, Twitter, Steam, Amazon und vermutlich sehr viele kleinere Angebote. Es gibt Proof-of-Concept-Code, der das Ausnutzen der Lücke demonstriert und auch bereits erste Angriffe. Seit Kurzem steht ein Quellcode-Update des Apache-Projekts bereit; Admins sollten dringend aktiv werden.Ein Angreifer kann die Sicherheitslücke ausnutzen, indem er manipulierte Anfragen an einen verwundbaren Server oder eine angreifbare Anwendung schickt.
Betroffen von der Sicherheitslücke ist Log4j von Version 2.0-beta9 bis 2.14.1. Das Apache-Projekt hat kurzfristig Version 2.15.0 veröffentlicht, die die Lücke schließt. In einer Sicherheitsmeldung listen die Apache-Entwickler zudem Maßnahmen auf, wie man die Server ohne Update vorläufig sichern kann. Bei Log4j ab Version 2.10 helfe das Setzen der Systemeigenschaft "log4j2.formatMsgNoLookups" auf “true” oder das Entfernen der JndiLookup-Klasse aus dem Klassenpfad (etwa mit dem Befehl zip -q -d log4j-core-*.jar org/apache/logging/log4j/core/lookup/JndiLookup.class).
Ein kleines Stück Software, bei fast jeder Software wird eingesetzt.
Lock for Jay, da kann man Schadcode ausführen auf dem betroffenen System
Grosse Teil auch von sehr grossen Firmen, also basieren auf Lock for Jay
Ist eine grosse Sicherheitslücke!
Nur wenige einzelne Community haben diese Software entwickelt.
Keiner zahlt für diese Software, sind Freiwillige.
Wie kann man es schaffen, für die GEmeinscahft entwickelte Software, diese zu finanzieren, ob da genug Entwickler da . 
Das ist ein Problem von Open Source haben, wenn sie eine gewisse Verbreitung haben,d ann muss man dann spätestens darüber nachdenken, wie man das pflegen kann.
Eine Multimilliondollarfirma soll zahlen und nicht jammern, wenn Lock for Jay nicht mehr tut, wenn sie Lock for Jay einsetzte, die fällt ja auch nicht vom Himmel, man sollte die freiwilligen Entwickler bezahlen.
Lock for Jay ist auch in VuFind. Isch ein wackliger Bauteil.
Aus vielen kleinen und noch kleineren Bauteilen, an einem kleinen tragenden Teil wird von einer Person aus Nebraska, dann bricht der ganze schöne Teil zusammen. Ist leider sehr nahe an der Realität dieses Bild wie man das bei Lock for Jay auch gesehen hat.

Die Universitätsbibliohtek Leipzig, man sieht sehr schnell wenn der Kern VUFInd ist.Die setzen auch VUFIND ein und ein paar andere open source software. Auch auf VuFind basierend






**Solr = nur reine Suchmaschine, aber kein Katalog oder Recherchemöglichkeit!**
Ist so quasi Industriestandard das Primo von Ex Libris basiert auf Solr, VuFind basiert auf SolR, SolR ist der Kern
Alle grossen Webseiten (die nicht gerade von Google kommen) die eine Suche anbieten basieren auf SolR
Haben wir auf jeden Fall als Anwender schon mal Kontakt haben.
Datumsfeld, alles andere lehnt er ab zb..
Ein Algorithmus kann viel schneller arbeiten, wenn er weiss dass die Daten immer einer ganz bestimmten Form z. b. Zeitintervall, Datum da geht die Suche dann viel Schneller
Darum muss man eben strikte Datentypen machen
 

 
**Daten reinstopfen und abfragen, was ist jetzt der Unterschied zwischen Suchindex (SolR) und einer Datenbank (My SQL)?**
Solr denkt nur in flachen Dokumente, MY SQL denkt in relationale Datensätze.
Man bekommt keine Teile in einem Objekt, bei MYSQL da ist das Objekt in einer Vielzahl von Daten und verweisen aufeinander. Man muss also mehrere Datensätze für Personen, Schlagwörter, immer eigenen Tabellen für Personen, eigene Tabellen für Objekte, man müsste alle Tabellen dann einzeln abfragen. Das ist auch durchaus gewollt bei relationalen Datenbanken.
Ein Suchindex, der würde den Autor mehrfach ablegen, weil er zu mehreren Objekten gehört.
Ein Suchindex ist stark in grammatische Grundform das Wort zurückzuführen, dass kann eine relationale Datenbank nicht. Eine relationale Datenbank nimmt genaue Zeichenfolge so an.
Aber ein Suchindex ist deutlich mächtiger er nimmt semantisch die Zeichenfolge ab.
Suchindex da findet keine Überprüfung statt, kann mehrere Datensätze reinmachen die gleichen. Bei relationale Datenbank my SQL muss redunant sein, es speichert auch, wenn er abstürzt.
Einmal in den Suchindex reingemachte Daten, lassen sich nicht mehr verändern, man kann nicht löschen. Wenn man Datensatz verändern möchte, dann tut er das löschen und ersetzt durch neuen.
Relationale Datensätze my sql tut die Datensätze mit den neuen Datensätzen verändern und nicht ersetzen.
Ein Suchindex bietet nur das Abrufen von Datenbanken an, eine my SQL-Datenbank also relationale kann alles machen CRUD : Create, Read, Update, Delete machen.

**Solr**
Backend von Solr
Spannend der Core SElector, mann kann verschiedene Datentopfe so pflegen also authority, biblio, reserves (vormerkungen), website (seine bibliothekswebseite über vufind). Core= Kerne
 250 Datensätze sind drin, meine Beispieldatensätze
 
	Max doch wären 500, wenn ich zweimal laden würde die Beispieldaten
Segment Count, mit jeder Importierung von Datensätze ist ein Segment. Also eine Datei so quasi. Alle Segmente werden so durchsucht, mit zunehmender Anzahl von Segmenten wird das mühsam. Bei uns sind jetzt 3 Segmente drin.

Kann diesen Suchindex jetzt auch durchsuchen mit Query
Schema gibt eine Unique Key field
Im Dropdown die verschiedenen Datenfelder Autoren…
Stored= diesen inhalt kann man aus dem index wieder angeben, stammformen durchsuchen, e
![Screenshot from 2021-12-16 09-52-12](https://user-images.githubusercontent.com/90834735/146373641-6f69a399-d4a9-4e69-94b5-45b139f50e05.png)


Multivalued= mehrere Autoren, die separat beinhaltet werden
Kann also mehrfache Werte speichern
 
Man kann mehrere isbn abrufen,Multivalued auch weil Autor gibt’s ja mehrere manchmal zu einem Buch
 
Welche felder lege ich an?
Wir werden ja dann unsere gesammelten Daten importieren marc feld 245 a landet dann im titelfeld!


Übung hinter die Kulissen schauen:  Vergleich mit  Terminal, Vergleich mit Backend von vufind und vufind Benutzeroberfläche
Control c und q=quit konnte ich wieder rausgehen
Terminal war dieser Befehl:  ist ein Programm das less


**Übung: Suche nach psychology bei Solr Admin, mit Terminal, SolrBenutzer**

hier die Ansicht in **SolrAdmin**:  http://localhost:8983/solr/#/biblio/query
allfields:psychology
![Screenshot from 2021-12-16 10-17-37](https://user-images.githubusercontent.com/90834735/146373957-96a63025-1c71-41f1-8392-2819f9472874.png)

 
**Wo sind Unterschiede?**

 
**SolrAdmin**: aktualisieren von Datensätze kann man über die blaue Versionsnummer, dann ändert sich auch die Nummer, ist so eine interne Verwaltung der Suchmaschine, ist für Suche selbst nicht so relevant. Aber es stimmt.



**SolrBenutzer**: 3 Suchanfragen sofort ersichtlich
**Terminal**: Um die Suchanfrage im Terminal mitzuverfolgen musste folgender Befehl eingegeben werden: 
less +F /usr/local/vufind/solr/vufind/logs/solr.log                   dieser konnte mit Control + C  und dann Q=quit wieder verlassen werden, man kan wieder auf das Start-Terminal zurück.
URL kommt da raus. Einige Anfragen. MAn musst fast Adleraugen haben!

 
Das ist Hauptanfrage.
Das ist Hintergrundanfrage..
 
Das ist aktuelle Anfrage.
 
Hits =3 , Status 0
Ja 3 Treffer bei allen 3.
 
Parameter mm
Dann kommt eigentlich Abfrage Q=psychology
Dann spell check unsw.
Dann eine ganze Meldung an Feldern gewählt 
Titel und Dach haben,Titel full 600 und Titel Full 400
Was sind das für Felder jetzt?
 
 **AHA-Moment: Ich kann auch das Ranking/Gewichtung im Terminal sehen!**
Titelfeld wichtiger als Autorenfeld, ja genau damit hängt es zusammen
Da haben wir ja nur 3 Treffer, einfaches Ranking
 Ist tatsächlich die Gewichtung / Ranking also wird mit 750 multipliziert, der kurztitel wird also sehr hoch gewichtet, dann kommt der volle Titel mit 600 Titel…
Den Alternativtitel mit 500
Autorenfeld mit 300
QF zählt alle Felder auf und gibt pro Feld noch die Gewichtung
darum auch Felder mit keiner 

Zahl, aber die werden noch mit 1 gewichtet.
hier der Screenshot dazu:

![Screenshot from 2021-12-16 12-45-43](https://user-images.githubusercontent.com/90834735/146366969-9db35fcd-bb1b-4564-a5b4-21f54a338e94.png)


 
 
	**AHA-Moment: Autoverfollständigung, rechts im Terminal kommt dann die Anfrage dazu gleichzeitig
![Screenshot from 2021-12-16 10-59-17](https://user-images.githubusercontent.com/90834735/146363848-c4267219-ab09-4cba-8f7d-48c84fac8fd3.png)
**: Im Terminal sehe ich bereits meine Suchanfrage, also die Autoverfollständigung der Suche diese ist parallel im Terminal ersichtlich hier der Screenshot!
	

 
**Finale übung**

Die dAten aus DSpace in Marc XML, Koha war schon Marc XML
Ziel: Import der Marc Edit mit Open Refine konvertierete Daten aus Koha, Archive Space, Dpsace und DOAJ in VUFind
Zuerst mussten wir die Testdaten löschen diese 250 Dateien.Jetzt haben wir wieder einen leeren Index.
Ich habe nicht meine eigenen gesammelten Daten von Koha, ARchiveSpace, DOJA und Dspace genommen, sondern die empfohlenen Beispieldaten, welche dann als Ordner
Koha, ARchiveSpace, DOJA, Dspace heruntergeladen werden konnten.
Dann mit dem Befehl **gedit /usr/local/vufind/import/marc_local.properties**   kam man auf die Marc Properties- Config-DAtei. Das war im Editor, dort musste man die Raute  wegnehmen, dann muss koha, dpace, archivespace, doaj jedes mal da einfügen bei der collection.

Danach mit dem Befehl **for f in ~/Downloads/koha/*.xml; do /usr/local/vufind/import-marc.sh $f; done*** konnte ich die gesammelten Beispieldaten aus koha, dpace, archivespace, Doaj raufladen ins VuFind.

 
Wir haben beide 11 Resultate bekommen aus den 4 Bib-und Archivsystemen:
Archive space hat nicht funktioniert
Koha funktioniert
Doaj funktioniert 
Dspace nicht funktioniert

**Warum nicht funktioniert?**

Jetzt einzelne Datensätze einzuspeisen
Alle in Marc xml konvertierten Daten
Ich nehme die Beispieldaten
Im Verzeichnis download mit archiv manager
Archive space ging nicht, ein ratespiel
Koha sollte gehen
Vor dem import muss datei noch in collection zuweisen

	
 
 
	Fehler: Unique field id = 001 hat gefehlt, nur bei koha und doaj war es vorhanden, bei anderen fiel es erst mit 245 an oder andere mit anderen marc-21 feldern, bei dspace war es 042, bei archive space fängt bie 008 an. aber 001 war bei dpace und archive space gar nicht belegt!

 

Archive space hier einzelne Marc Felder aber fängt bie 008 an, oo1 ist gar nicht belget

D Space erstes Feld ist 042

 
Koha 001 hier sehe ich das Feld, darum hat es auch importiert
 

Eine eindeutige ID ist das Pflichtfeld, sonst geht Datenimport nicht.

Problem lösen

**Erste Lösung**: **In Texteditor also die 001 manuell reinfügen, aber sicher nicht für viele DAtensätze.**
 

**Zweite Lösung: Oder wir machen in den marc properties**
Nimm titelfeld und nicht 001, aber es würde ein problem geben dann weil titel ist nicht eindeutiger Identifier. Also müsste man beim Export da was anderes machen, dass das feld 001 mitgeführt wird!
  
	Titel wird mit245 befüllt
Und untertiel mit 245 a
Vufind hat also Daten  zusätzlich angereichert!.





**Finale übung Ergebnisse**


![Screenshot from 2021-12-16 12-01-20](https://user-images.githubusercontent.com/90834735/146375636-fba8f775-5bca-4189-893d-3ad3deae7916.png)



Die Institution habe wieder reingenommen bei den FAcetten, da bin ich nochmals in die Config-Dateien gegangen.
Nun mein Screenshot hier. Es waren 11 Ergebnisse. **Die Dateien von Koha sind drin und die von Doaj**.


**Ursache für Fehler des Imports der ArchivesSpace und DSpace Daten?**
id ist nicht vorhanden (Fehlermeldung - id: null & Document is missing mandatory uniqueKey field: id)

Die Dateien von DSpace und Archive Space hat es nicht genommen, weil eben dieses Feld001 nicht ausgefüllt war.


