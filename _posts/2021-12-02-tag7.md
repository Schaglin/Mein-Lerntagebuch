**Tag 7 Transformation von Metadaten mit Open Refine**

Metadata von Beginn an zu zu schaffen
•	Transformation (Veränderung) von Metadaten mit OpenRefine
•	Weitere Tools zur Metadatentransformation


Normalerweise Nutzung von JSON-APIs   wenn nicht im Archiv oder Bibliothek


Metadata-Modellieren: Mapping  sind ja diese Regeln, um ein Format in ein anders zu übertragen

Naturkunde Museum, war der Hr. Lohmeier. Sammlung noch in Exceltabellen, solange begrenzte Anzahl von Datensätzen. Jetzt wollen sie das gerne zusammenführen, wie kriege ich jetzt diese unstrukturierten daten in dublin core oder marc21?

OpenRefine unabhängig von  Koha etc. nicht nur in Bibliothek oder Archiv.
Claim: a free open source, powerful tool for working with messy data.
Hat Bibliothek keine messy data, aber doch auch Tippfehler hat es drin, auch trotzt Regelwerk sind nicht fehlerfrei. Bei der Erschliessung sind die Daten vielleicht besser als Amazon aber passieren doch Fehler.Ein mächtiges Tool.
Das besondere an Open Refine ein Mischling aus Tabellenverarbeitungssoftware.
Open Refine ist interessant, die nicht gerne mit Terminal arbeiten wollen.Aber doch sich zutrauen, einzelne kleine Scripte zu schreiben. Sieht aus wie Excel. Man kann Fehler bereinigen, in versch. Daten exportieren und auch externe Datenquellen heranziehen.
Schnittstelle zur gemeinsamen Normdatei. Wenn Geburtsort und Sterbeort noch ergänzen.
Wird in der Regel lokal auf Computer installiert, wird aber über Browser bedient.
Es ist aber keine Cloud-Software. Sind denn meine Daten sicher? Ja, als ob ich kleinen Webserver auf meinem Computer installiere.
Es gehen keine Daten nach draussen, ausser ich setzte externe Schnittstellen ein.

 
Also am meisten wird es eingesetzt zur Bereinigung von Tippfehlern.
z. B. Die sollen von Koha in Alma gehen und wenn jetzt der Koha Export nicht perfekt, ist, kann es mit open refine gemacht werden.
Understanding data you don’t know. Museum, Archive gibt uns Daten rüber, dann Hr. Lohmeier erste Schritt geht ins open refine, wenn steht Marc21, kann man nicht darauf vertrauen, dass es auch Marc21 ist.
Reconciliation: Nutzung externe Datenquelle, um eigene Daten anzureichern.


 Bibliotheken : Marc Edit ist zwar naheliegender für Marc21, aber open refine geht auch, bevor sie dann ins Bibliotheksystem integriert werden.
Data Sience, Semantic Web, Consulting,Business , Research, aber am moisten doch Bibliotheken. Aber auch gutes Zeichen, wenn diese Software von Data Science arbeiten.

Von Open Refine unterstützte Daten: Besonders gut mit Tabellendaten
(CSV, TSV, XLS, XLSX auch TXT)
XML geht auch wenn ein flaches ist wie MARCXML
JSOn ist mit etwas Übung einfach zu modellieren
XML kompex wie EAD ist schwieriger mit Zusatztool   (Lohmeier für Marbach literaturarchiv gemacht)
Kann auch mit Marc Edit, weil hat dort Exportfunktion für Marc Daten für open refine, dann kann man sie besser in open refine transformieren, dann wieder in Marc Edit zurückmachen.
Open refine ist gut für Massendaten, darum hat Terry Reese dieses Tool auch eingesetzt

Einsatzmöglichkeiten
Vereinheitlichung und Bereinigung     
126 Mio von Nationaldatenbibliotheken hat Peter Kiraly verglichen, da hat es Fehlerquoten von 20 % ,der den Marc21? Datensätzen widerspricht.
Verlagsdaten wo kein geschultes Personal, dann noch migriert, dann hats Fehler zusammen.
Schlimme Fehler z.b. ist falsche Marc Feld verwendet worden Titel statt Autor
Abgleich mit Normdaten (Renconciliation) in Wikidata, GND und VIAF
Für lokalen Einsatz ausgelegt (Installation auf Webserver, Zusatzsoftware)
 
Grosse Entwicklercommunity, auf github
Von Firma metaweb, ist von google aufgekauft worden google refine
Google keine Verwendung mehr, dann wurde es open source
98 verschiedene Menschen, haben zu open refine beigetragen (Auch Übersetzungen)
 
Vufind hat nur 212 Sterne

5min hat er auto Save
 
Get und post der redet mit dem browser
Wenn open refine ausgeschaltet ist, kann ich logischerweise auch im browser bedienen
 
Open refine hat erkannt, dass es csv data sind
Beliebiges trennzeichen geht auch
Oder tabs oder commas
Bei import soll man aber möglichst wenig tun (automatismen) Weil wird in änderungshistorie sehen
Aber leerzeichen in tabellen hat oder spalten gehen über 2 zeilen, dann kann man das einstellen
Die ersten100 Datensätze werden mal angezeigt, da soll man schauen, ob alles Sonderzeichen é und so auch übernommen wurden und richtig dargestellt sind. Wenn merkwürdiges Fragezeichen steht, dann gäbe es ein Problem. Aber meisten Datensätze sind in unicode oder  utf8
 
Wenn standard utf8 nicht funktioniert, müsste man hier anpassen
Dann auf create project, dann kommt hauptbildschirm (projekte) kann nun arbeiten mit den Daten
Ansicht erinnert schon an Excel, aber nur ersten 10 Datensätze sieht man
Anzahl zeilen möchte ersten 50 sehen
 

Vorteil eben grosse Datenmengen, Excel da schwieriger
Früher gabe es nur 246 Spalten und 65'000 Zeilen, gilt zwar heute nicht mehr, war aber noch 2013 der Fall
 
 
Für änderungshistorie
 






EN und Englisch das ist ungleich, diese könnte man jetzt ändern

 
 einmal leer, 871 xEN, 107x English
 
	 
	107 sind nun gändert worden von Englisch zu EN, sehe im undo/redo
 

Transformation da  wegen den änderungen
Achtung, dass nicht facette aufhabe
 
7 sieht man hier, die spanischen
 
	Achtung, dass nicht noch ein Filter oder Facette aktiv habe, ich soll immer als Orientierung schauen im Titel, dass nicht noch eine Facette aktiv war.

Kann auch ein Sprache exkludieren französich und englisch
 
 
	Yoshinowo als einzelautor kam 7 x vor, aber man weiss nicht, ob in mehrfach solche autoren, noch häuifger waren
 
 
	Mehrfachbelegung
 


 
3000 können wir bestätigen
Dann bekommt man die Autorennamen
 
 
	
 

 
	Hier kann Leerzeichen einfach entfernen
Mal werden Namen abgekürzt, mal nicht, gibt Cluster Button dafür
 
	Gibt Vorschläge vom Computer wo Ähnlichkeiten sind, hat Algorithmus gemacht, jetzt kann ich diese Vorschläge (Cluster) nehmen oder nicht.
Einmal Vorname, einmal
Dann einmal Nachname
 
Hier fehlt ein Punkt beim K





9 mal kommt es vor, dann ist das andere ein Tippfehler, das nehmen wo häufiger vorkommt
 
	 
	 
	Die ausgewählten werden neu berechnet, kann aber sein dass noch mehr cluster entstehen
Nearest neighbor wieviele schritte, dass abweichung kommt (anderer Clustervorschlag)
 
	

Datensätze wieder zusammenführen
In zeilen arbeiten, nur bei bedarf aufsplitten
Gibt noch record ansicht
 
 
	Jetzt werden die Autoren, wo vorher gesplitten waren, wieder zusammengefügt die Autoren, man soll sich Zahl 1001 merken von vorher, jetzt auch wieder 1001 sind
 
 
Auf Apply gehen und das Leerzeichen rausnehmen
 
Jetzt erscheint nur einmal

MDPI AG hat Leerzeichen drin, dann erscheint zweimal, muss also rausnehmen
 
	
 
	Cluster hätte man nutzen sollen




Wikidata gibt es einträge in der datenbanken für zeitschriften
Abgleich machen zu können, brauchen wir einen namen
 
Neue spalte anlegen, basierend auf neuer spalte (excel ist einfacher)
 
	 
Kann if und for schleifen, kann so komplexe transformationen machen
Könnte im libary carpentry lernen
Im wiki (mit kochrezepten?) kann mit isbn 10 auf isbn 13 kann das mit copy paste einfach anwednden
Grell sprache um nur den zeitschriftenname zu ändern
Split funktion wird wieder gebraucht
Value ist Standarwert , übernimmt aus der voherigen Spalte
value.split(",")[0]
Value. S plit («,»)   ()= Funktion  ,aber Eckige Klammer beginnt bei 0, wollenw aus dem Ergebnis nur ersten Teil
 

	 
Oder ich kann klicken, wenn nicht grell funktion schreiben möchte
 
Neue Spalte Journal ist erschienen

Reconcilation (Fachbegrifff für Abgleich externe Daten )
Journal klicken, dann Start Reconciliation
 
	 
	Man könnte in fragekommende datensätze auf diesen typ einschränkt, aber es können auch vorschläge verloren gehen, jetzt im wikidata ein paar
Datensätze anschauen, denn nicht alle datensäzte sind diesen journals eingeordet,
dann soll man lieber allgemeine kategorie drin lassen
 
	Kein bestimmter typ

Mensch, dass nicht ortschaft kommt?
 
 
	Start reconciling
Sind 1001 Datensätze aber nicht viele werte, 51 verschiedene werte
Im Hintergrund wird jetzt ein Dienst angefragt, und open refine ermittelt jetzt für spalte Journal aus den datensätzen von wikidata.
www.wikidata.org
 
	 
 
Offical webseite
Gibt noch 2. Issn für printausgabe
 
 
	 
	 
Bessere Vorschläge, kann nicht nur mit Zeitschriftennamen kann auch mit Personennamen und geburtsdatum im wikidata abgleichen
 
Jetzt gibt’s die Match aus dem Wikidata für bestimmte Zeitschriften 
Nicht überall
 
952 wurden gematcht mit wikidata
 
 
Mach all identical cells
 
	Kann match zuordnen 
	 

 
 
 
	
 
 
	Scopus ID der Abgleich erscheint jetzt
 
	

Man könnte bei Personennamen mit viaf abgleichen
 

	

CSV nach MARC XML : Modellieren im marc xml exportierne
Wir wollen jetzt marc xml produzieren, vorher war es in json json voreinstellung. Wenn schliesse geht alles verloren
 
	Marc xml  
	 
	Das ist struktur wie wir machen müssten
Wenn man so ein template schreibt, könnte man für alle 1000 nehemn
Aber wir können nur für autor oder titel machen
Namespaces hat er marc doppelpunkt entfernt
 
Template geht für alle Datensätze, wenn zwei geschweiffte klammernn stehen, da kommen dnan die inhalte rein, cells url also der inhalt der spalte url wird dann übernommen und https doa.org/artickle wird erstetze nur identifier bleibt stehen über noch ein punkt escape dass alles xml entspricht. Weil xml erlaubt keine umlaute, diese müssen entfernt werden
Gibt auch eine for- schleife- für mehrer autoren
Jetzt muss in gruppenarbeit 
reverse engineering
erste feld für 001, ist schon geschireben
feld issn, die umlaute ja aber
autoren
 
Split 0 funktion was die mit Autor macht,
erste wert wird nur genommen, es wird also flavia genommen und das wird in dieses feld 100a geschreiben und dnan wieder ins xml kopiert
 
Autor 700 also diese slice schneidet den wert weg, diese wird dann in wert ins subfield code a genommen
 
	 
	Template ergänzen
Marc Felder füllen
 
Bis jetzt autor, titel, url, issn genommen


Jetzt noch die doi nehmen
 
	Zuerst muss mal doi spalte anschauen, 73 
 wo es keine doi hat
Muss in geschweiffte klammern um abzugrenzen
 
	Gibt nur null, dann müsste jetzt eine if abfrage, wenn das feld leer, dann mach das
Oder man kann nonblade
  wens blank ist, nichts reinschreiben
 
 
	Wenns blank ist, dann nicht, und sonst datafield
 
	 
	Wenn datensäzte leer sind, dann nichts und sonst doi
 

 
	Wenn das feld nicht leer ist, dann doi schreiben in feld 024? Unterfeld a2


 
	
 
So haben wir schon fast gültige datensätze, diese müsste man noch validieren mit xmilinit, kann man prüfen ob es dem schema marc21 entspricht, hatten wir im gset
 

	 
Muss noch umbenennen in xml
 

	Ead gibt’s im marc21 nicht
 
	Validierung ist wichtig auch in Nationalbibliotheken, prüft ob Reihenfolge und Zahlen und keine Buchstaben und ob das alles passt. Weil Daten manuell, gibt viele Fehler.

Validierung in der SHELL mit diesen Befehlen:
Metadata von Beginn an zu zu schaffen
•	Transformation (Veränderung) von Metadaten mit OpenRefine
•	Weitere Tools zur Metadatentransformation


Normalerweise Nutzung von JSON-APIs   wenn nicht im Archiv oder Bibliothek


Metadata-Modellieren: Mapping  sind ja diese Regeln, um ein Format in ein anders zu übertragen

Naturkunde Museum, war der Hr. Lohmeier. Sammlung noch in Exceltabellen, solange begrenzte Anzahl von Datensätzen. Jetzt wollen sie das gerne zusammenführen, wie kriege ich jetzt diese unstrukturierten daten in dublin core oder marc21?

OpenRefine unabhängig von  Koha etc. nicht nur in Bibliothek oder Archiv.
Claim: a free open source, powerful tool for working with messy data.
Hat Bibliothek keine messy data, aber doch auch Tippfehler hat es drin, auch trotzt Regelwerk sind nicht fehlerfrei. Bei der Erschliessung sind die Daten vielleicht besser als Amazon aber passieren doch Fehler.Ein mächtiges Tool.
Das besondere an Open Refine ein Mischling aus Tabellenverarbeitungssoftware.
Open Refine ist interessant, die nicht gerne mit Terminal arbeiten wollen.Aber doch sich zutrauen, einzelne kleine Scripte zu schreiben. Sieht aus wie Excel. Man kann Fehler bereinigen, in versch. Daten exportieren und auch externe Datenquellen heranziehen.
Schnittstelle zur gemeinsamen Normdatei. Wenn Geburtsort und Sterbeort noch ergänzen.
Wird in der Regel lokal auf Computer installiert, wird aber über Browser bedient.
Es ist aber keine Cloud-Software. Sind denn meine Daten sicher? Ja, als ob ich kleinen Webserver auf meinem Computer installiere.
Es gehen keine Daten nach draussen, ausser ich setzte externe Schnittstellen ein.

 
Also am meisten wird es eingesetzt zur Bereinigung von Tippfehlern.
z. B. Die sollen von Koha in Alma gehen und wenn jetzt der Koha Export nicht perfekt, ist, kann es mit open refine gemacht werden.
Understanding data you don’t know. Museum, Archive gibt uns Daten rüber, dann Hr. Lohmeier erste Schritt geht ins open refine, wenn steht Marc21, kann man nicht darauf vertrauen, dass es auch Marc21 ist.
Reconciliation: Nutzung externe Datenquelle, um eigene Daten anzureichern.


 Bibliotheken : Marc Edit ist zwar naheliegender für Marc21, aber open refine geht auch, bevor sie dann ins Bibliotheksystem integriert werden.
Data Sience, Semantic Web, Consulting,Business , Research, aber am moisten doch Bibliotheken. Aber auch gutes Zeichen, wenn diese Software von Data Science arbeiten.

Von Open Refine unterstützte Daten: Besonders gut mit Tabellendaten
(CSV, TSV, XLS, XLSX auch TXT)
XML geht auch wenn ein flaches ist wie MARCXML
JSOn ist mit etwas Übung einfach zu modellieren
XML kompex wie EAD ist schwieriger mit Zusatztool   (Lohmeier für Marbach literaturarchiv gemacht)
Kann auch mit Marc Edit, weil hat dort Exportfunktion für Marc Daten für open refine, dann kann man sie besser in open refine transformieren, dann wieder in Marc Edit zurückmachen.
Open refine ist gut für Massendaten, darum hat Terry Reese dieses Tool auch eingesetzt

Einsatzmöglichkeiten
Vereinheitlichung und Bereinigung     
126 Mio von Nationaldatenbibliotheken hat Peter Kiraly verglichen, da hat es Fehlerquoten von 20 % ,der den Marc21? Datensätzen widerspricht.
Verlagsdaten wo kein geschultes Personal, dann noch migriert, dann hats Fehler zusammen.
Schlimme Fehler z.b. ist falsche Marc Feld verwendet worden Titel statt Autor
Abgleich mit Normdaten (Renconciliation) in Wikidata, GND und VIAF
Für lokalen Einsatz ausgelegt (Installation auf Webserver, Zusatzsoftware)
 
Grosse Entwicklercommunity, auf github
Von Firma metaweb, ist von google aufgekauft worden google refine
Google keine Verwendung mehr, dann wurde es open source
98 verschiedene Menschen, haben zu open refine beigetragen (Auch Übersetzungen)
 
Vufind hat nur 212 Sterne

5min hat er auto Save
 
Get und post der redet mit dem browser
Wenn open refine ausgeschaltet ist, kann ich logischerweise auch im browser bedienen
 
Open refine hat erkannt, dass es csv data sind
Beliebiges trennzeichen geht auch
Oder tabs oder commas
Bei import soll man aber möglichst wenig tun (automatismen) Weil wird in änderungshistorie sehen
Aber leerzeichen in tabellen hat oder spalten gehen über 2 zeilen, dann kann man das einstellen
Die ersten100 Datensätze werden mal angezeigt, da soll man schauen, ob alles Sonderzeichen é und so auch übernommen wurden und richtig dargestellt sind. Wenn merkwürdiges Fragezeichen steht, dann gäbe es ein Problem. Aber meisten Datensätze sind in unicode oder  utf8
 
Wenn standard utf8 nicht funktioniert, müsste man hier anpassen
Dann auf create project, dann kommt hauptbildschirm (projekte) kann nun arbeiten mit den Daten
Ansicht erinnert schon an Excel, aber nur ersten 10 Datensätze sieht man
Anzahl zeilen möchte ersten 50 sehen
 

Vorteil eben grosse Datenmengen, Excel da schwieriger
Früher gabe es nur 246 Spalten und 65'000 Zeilen, gilt zwar heute nicht mehr, war aber noch 2013 der Fall
 
 
Für änderungshistorie
 






EN und Englisch das ist ungleich, diese könnte man jetzt ändern

 
 einmal leer, 871 xEN, 107x English
 
	 
	107 sind nun gändert worden von Englisch zu EN, sehe im undo/redo
 

Transformation da  wegen den änderungen
Achtung, dass nicht facette aufhabe
 
7 sieht man hier, die spanischen
 
	Achtung, dass nicht noch ein Filter oder Facette aktiv habe, ich soll immer als Orientierung schauen im Titel, dass nicht noch eine Facette aktiv war.

Kann auch ein Sprache exkludieren französich und englisch
 
 
	Yoshinowo als einzelautor kam 7 x vor, aber man weiss nicht, ob in mehrfach solche autoren, noch häuifger waren
 
 
	Mehrfachbelegung
 


 
3000 können wir bestätigen
Dann bekommt man die Autorennamen
 
 
	
 

 
	Hier kann Leerzeichen einfach entfernen
Mal werden Namen abgekürzt, mal nicht, gibt Cluster Button dafür
 
	Gibt Vorschläge vom Computer wo Ähnlichkeiten sind, hat Algorithmus gemacht, jetzt kann ich diese Vorschläge (Cluster) nehmen oder nicht.
Einmal Vorname, einmal
Dann einmal Nachname
 
Hier fehlt ein Punkt beim K





9 mal kommt es vor, dann ist das andere ein Tippfehler, das nehmen wo häufiger vorkommt
 
	 
	 
	Die ausgewählten werden neu berechnet, kann aber sein dass noch mehr cluster entstehen
Nearest neighbor wieviele schritte, dass abweichung kommt (anderer Clustervorschlag)
 
	

Datensätze wieder zusammenführen
In zeilen arbeiten, nur bei bedarf aufsplitten
Gibt noch record ansicht
 
 
	Jetzt werden die Autoren, wo vorher gesplitten waren, wieder zusammengefügt die Autoren, man soll sich Zahl 1001 merken von vorher, jetzt auch wieder 1001 sind
 
 
Auf Apply gehen und das Leerzeichen rausnehmen
 
Jetzt erscheint nur einmal

MDPI AG hat Leerzeichen drin, dann erscheint zweimal, muss also rausnehmen
 
	
 
	Cluster hätte man nutzen sollen




Wikidata gibt es einträge in der datenbanken für zeitschriften
Abgleich machen zu können, brauchen wir einen namen
 
Neue spalte anlegen, basierend auf neuer spalte (excel ist einfacher)
 
	 
Kann if und for schleifen, kann so komplexe transformationen machen
Könnte im libary carpentry lernen
Im wiki (mit kochrezepten?) kann mit isbn 10 auf isbn 13 kann das mit copy paste einfach anwednden
Grell sprache um nur den zeitschriftenname zu ändern
Split funktion wird wieder gebraucht
Value ist Standarwert , übernimmt aus der voherigen Spalte
value.split(",")[0]
Value. S plit («,»)   ()= Funktion  ,aber Eckige Klammer beginnt bei 0, wollenw aus dem Ergebnis nur ersten Teil
 

	 
Oder ich kann klicken, wenn nicht grell funktion schreiben möchte
 
Neue Spalte Journal ist erschienen

Reconcilation (Fachbegrifff für Abgleich externe Daten )
Journal klicken, dann Start Reconciliation
 
	 
	Man könnte in fragekommende datensätze auf diesen typ einschränkt, aber es können auch vorschläge verloren gehen, jetzt im wikidata ein paar
Datensätze anschauen, denn nicht alle datensäzte sind diesen journals eingeordet,
dann soll man lieber allgemeine kategorie drin lassen
 
	Kein bestimmter typ

Mensch, dass nicht ortschaft kommt?
 
 
	Start reconciling
Sind 1001 Datensätze aber nicht viele werte, 51 verschiedene werte
Im Hintergrund wird jetzt ein Dienst angefragt, und open refine ermittelt jetzt für spalte Journal aus den datensätzen von wikidata.
www.wikidata.org
 
	 
 
Offical webseite
Gibt noch 2. Issn für printausgabe
 
 
	 
	 
Bessere Vorschläge, kann nicht nur mit Zeitschriftennamen kann auch mit Personennamen und geburtsdatum im wikidata abgleichen
 
Jetzt gibt’s die Match aus dem Wikidata für bestimmte Zeitschriften 
Nicht überall
 
952 wurden gematcht mit wikidata
 
 
Mach all identical cells
 
	Kann match zuordnen 
	 

 
 
 
	
 
 
	Scopus ID der Abgleich erscheint jetzt
 
	

Man könnte bei Personennamen mit viaf abgleichen
 

	

CSV nach MARC XML : Modellieren im marc xml exportierne
Wir wollen jetzt marc xml produzieren, vorher war es in json json voreinstellung. Wenn schliesse geht alles verloren
 
	Marc xml  
	 
	Das ist struktur wie wir machen müssten
Wenn man so ein template schreibt, könnte man für alle 1000 nehemn
Aber wir können nur für autor oder titel machen
Namespaces hat er marc doppelpunkt entfernt
 
Template geht für alle Datensätze, wenn zwei geschweiffte klammernn stehen, da kommen dnan die inhalte rein, cells url also der inhalt der spalte url wird dann übernommen und https doa.org/artickle wird erstetze nur identifier bleibt stehen über noch ein punkt escape dass alles xml entspricht. Weil xml erlaubt keine umlaute, diese müssen entfernt werden
Gibt auch eine for- schleife- für mehrer autoren
Jetzt muss in gruppenarbeit 
reverse engineering
erste feld für 001, ist schon geschireben
feld issn, die umlaute ja aber
autoren
 
Split 0 funktion was die mit Autor macht,
erste wert wird nur genommen, es wird also flavia genommen und das wird in dieses feld 100a geschreiben und dnan wieder ins xml kopiert
 
Autor 700 also diese slice schneidet den wert weg, diese wird dann in wert ins subfield code a genommen
 
	 
	Template ergänzen
Marc Felder füllen
 
Bis jetzt autor, titel, url, issn genommen


Jetzt noch die doi nehmen
 
	Zuerst muss mal doi spalte anschauen, 73 
 wo es keine doi hat
Muss in geschweiffte klammern um abzugrenzen
 
	Gibt nur null, dann müsste jetzt eine if abfrage, wenn das feld leer, dann mach das
Oder man kann nonblade
  wens blank ist, nichts reinschreiben
 
 
	Wenns blank ist, dann nicht, und sonst datafield
 
	 
	Wenn datensäzte leer sind, dann nichts und sonst doi
 

 
	Wenn das feld nicht leer ist, dann doi schreiben in feld 024? Unterfeld a2


 
	
 
So haben wir schon fast gültige datensätze, diese müsste man noch validieren mit xmilinit, kann man prüfen ob es dem schema marc21 entspricht, hatten wir im gset
 

	 
Muss noch umbenennen in xml
 

	Ead gibt’s im marc21 nicht
 
	Validierung ist wichtig auch in Nationalbibliotheken, prüft ob Reihenfolge und Zahlen und keine Buchstaben und ob das alles passt. Weil Daten manuell, gibt viele Fehler.


**Validierung in der SHELL /TERMINAL**
![Screenshot from 2021-12-03 12-19-13](https://user-images.githubusercontent.com/90834735/144594143-1f1a6cb5-4490-4a95-965a-b50adf2d943d.png)


 
	Dann habe es validiert, da hat es dann aber alle dokumente genommen,  aber man kann sehen, dass der doaj-article-sample-csv.xml validates geklappt hat das validieren, bei den anderen downloads die auch noch drin waren, hat die validierug nicht geklappt

Open refine speichert alles, auch wenn beednet habe


 im Ordner downloads hat es noch andere Dokumente, diese wurden halt auch mit validiert
 
	Dann habe es validiert, da hat es dann aber alle dokumente genommen,  aber man kann sehen, dass der doaj-article-sample-csv.xml validates geklappt hat das validieren, bei den anderen downloads die auch noch drin waren, hat die validierug nicht geklappt

Open refine speichert alles, auch wenn beednet habe
