---
title: "Tag 7"
date: 2021-12-02
---
_Liebes Tagebuch_,

**Transformation von Metadaten mit Open Refine**

Wir lernen heute: 
-Metadata von Beginn an zu zu schaffen
-Transformation (Veränderung) von Metadaten mit OpenRefine
-Weitere Tools zur Metadatentransformation

**JSON-API's**
Normalerweise Nutzung von JSON-APIs, wenn nicht im Archiv oder Bibliothekswesen. JSON ist neuer als Marc XML.
Ich muss zuerst mal googlen, was jetzt mit JSON-API genau gemeint ist: https://jsonapi.org/
aha, ja war ja diese Programmiersprache, das hatten wir schon einmal etwas dazu.



**Metadata-Modellieren: Mapping  sind ja diese Regeln, um ein Format in ein anders zu übertragen**

Naturkunde Museum, war der Hr. Lohmeier. Die Sammlung war da noch in Exceltabellen. Mit Excel geht das schon, solange begrenzte Anzahl von Datensätzen besteht. Jetzt wollen sie das gerne zusammenführen, wie kriegt man diese unstrukturierten Daten aus dem Excel in dublin core oder marc21?
Die Lösung heisst OpenRefine!

**OpenRefine**
Open Refine ist unabhängig von Koha etc. und wird nicht nur in Bibliothek oder Archiven angewendet.
Open Refine beschreibt sich als **_a free open source, powerful tool for working with messy data_**
Hat Bibliothek keine messy data, aber doch auch Tippfehler hat es drin, auch trotzt Regelwerk sind nicht fehlerfrei. Bei der Erschliessung sind die Daten vielleicht besser als Amazon aber passieren doch Fehler.Ein mächtiges Tool.
Das besondere an Open Refine ein Mischling aus Tabellenverarbeitungssoftware.
Open Refine ist interessant, die nicht gerne mit Terminal arbeiten wollen.Aber doch sich zutrauen, einzelne kleine Scripte zu schreiben. Sieht aus wie Excel. Man kann Fehler bereinigen, in versch. Daten exportieren und auch externe Datenquellen heranziehen.
Schnittstelle zur gemeinsamen Normdatei. Wenn Geburtsort und Sterbeort noch ergänzen.
Wird in der Regel lokal auf Computer installiert, wird aber über Browser bedient.
Es ist aber keine Cloud-Software. Sind denn meine Daten sicher? Ja, als ob ich kleinen Webserver auf meinem Computer installiere.
Es gehen keine Daten nach draussen, ausser ich setzte externe Schnittstellen ein.

Open Refine wird also am meisten wird es eingesetzt zur Bereinigung von Tippfehlern.
z. B. Die sollen von Koha in Alma gehen und wenn jetzt der Koha Export nicht perfekt, ist, kann es mit open refine gemacht werden.

**Understanding data you don’t know**. 
Also ein Museum, Archive gibt uns Daten rüber, dann Hr. Lohmeier erste Schritt geht ins open refine, wenn steht Marc21, kann man nicht darauf vertrauen, dass es auch Marc21 ist.

**Reconciliation**: Nutzung externe Datenquelle, um eigene Daten anzureichern.
Abgleich mit Normdaten (Renconciliation) in Wikidata, GND und VIAF.

**Bibliotheken**: Marc Edit ist zwar naheliegender für Marc21, aber open refine geht auch, bevor sie dann ins Bibliotheksystem integriert werden.
Data Sience, Semantic Web, Consulting,Business , Research, aber am meisten doch Bibliotheken. Aber auch gutes Zeichen, wenn diese Software von Data Science arbeiten.

**Welche Formate gehen gut mit Open Refine?**
Von Open Refine unterstützte Daten: Besonders gut mit Tabellendaten
(CSV, TSV, XLS, XLSX auch TXT)
XML geht auch wenn ein flaches ist wie MARCXML
JSOn ist mit etwas Übung einfach zu modellieren
XML kompex wie EAD ist schwieriger mit Zusatztool   (Lohmeier für Marbach literaturarchiv gemacht)
Kann auch mit Marc Edit, weil hat dort Exportfunktion für Marc Daten für open refine, dann kann man sie besser in open refine transformieren, dann wieder in Marc Edit zurückmachen.
Open refine ist gut für Massendaten, darum hat Terry Reese dieses Tool auch eingesetzt.

**Einsatzmöglichkeiten**
Vereinheitlichung und Bereinigung     
126 Mio von Nationaldatenbibliotheken hat Peter Kiraly verglichen, da hat es Fehlerquoten von 20 % ,der den Marc21? Datensätzen widerspricht.
Verlagsdaten wo kein geschultes Personal, dann noch migriert, dann hats Fehler zusammen.
Schlimme Fehler z.b. ist falsche Marc Feld verwendet worden Titel statt Autor.
Abgleich mit Normdaten (Renconciliation) in Wikidata, GND und VIAF.
Für lokalen Einsatz ausgelegt (Installation auf Webserver, Zusatzsoftware)
 
Open Refine hat eine grosse Entwicklercommunity, auf github
Von Firma metaweb, ist von google aufgekauft worden google refine.
Google hat keine Verwendung mehr für google refine, dann wurde es open source.
98 verschiedene Menschen, haben zu open refine beigetragen (Auch Übersetzungen)
Vufind hat nur 212 Sterne, open Refine hat mehr Sterne.
5min hat er auto Save
Get und post der redet mit dem browser
Wenn open refine ausgeschaltet ist, kann ich logischerweise auch im browser bedienen.

**Installation open refine**
open refine wurde dann mit copy-/paste Befehlen installiert und dann konnte sie eben im erwähnten browser aufrufen (http://localhost:3333/)
Das klappte auf Anhieb! :-)

 
**Vorstellung open refine und  einfache Funktionen wurden erklärt:**
Open refine hat erkannt, dass es csv data sind
Beliebiges Trennzeichen geht auch
Oder Tabs oder Kommas.
Beim Import soll man aber möglichst wenig tun, weil es Automatismen hat. Weil man wird in Änderungshistorie sehen.
Aber Leerzeichen in Tabellen hat oder Spalten gehen über 2 Zeilen, dann kann man das einstellen.
Die ersten 100 Datensätze werden mal angezeigt, da soll man schauen, ob alles Sonderzeichen é und so auch übernommen wurden und richtig dargestellt sind. Wenn merkwürdiges Fragezeichen steht, dann gäbe es ein Problem. Aber meisten Datensätze sind in unicode oder utf8.
 
![Screenshot from 2021-12-03 09-19-27](https://user-images.githubusercontent.com/90834735/144598224-cc2af6b4-5908-4e61-a1d1-377e611a155c.png)

Wenn Standard utf8 nicht funktioniert, müsste man hier anpassen. Dann auf **create project**, dann kommt Hauptbildschirm (Projekte) kann nun arbeiten mit den Daten.Ansicht erinnert schon an Excel, aber nur ersten 10 Datensätze sieht man.
Anzahl Zeilen möchte ersten 50 sehen.
 

Vorteil eben grosse Datenmengen, Excel da schwieriger im Excel gab es früher 246 Spalten und 65'000 Zeilen, gilt zwar heute nicht mehr, war aber noch 2013 der Fall.

**Aufgaben wurden gemacht*:
![EN](https://user-images.githubusercontent.com/90834735/150561878-eceaaa77-9387-4552-a32d-cbd74bebbc12.png)

**AHA-Moment: EN und Englisch ist ja ungleich, wo hat es mehr bei EN hat es mehr, also kann ganz einfach im Edit ändern. einmal leer, 871 xEN, 107x English


dann sehe ich 107 solcher Englisch in EN sind nun gändert worden von Englisch zu EN, 

**undo/redo**
Es gibt eine Speicherung des Verlaufes: undo/redo
sehe im undo/redo alle Transformationen und Änderungen. Wenn ein Fehler passiert, kann einfach redo machen.

Achtung, dass nicht noch ein Filter oder Facette aktiv habe, ich soll immer als Orientierung schauen im Titel, dass nicht noch eine Facette aktiv war.

7 sieht man hier, die spanischen Autoren /Zeitschriften.
Kann auch ein Sprache exkludieren französich und englisch.
 
 
Yoshinowo als Einzelautor kam 7 x vor, aber man weiss nicht, ob in mehrfach solche autoren, noch häufiger waren: ACHTUNG vor Mehrfachbelegung.
3000 können wir bestätigen
Dann bekommt man die Autorennamen
 
 
	

**Clusterig und Algorithmen, die Namensvorschläge bringen im open refine**:
Hier kann Leerzeichen einfach entfernen
Mal werden Namen abgekürzt, mal nicht, gibt Cluster Button dafür
Gibt Vorschläge vom Computer wo Ähnlichkeiten sind, hat Algorithmus gemacht, jetzt kann ich diese Vorschläge (Cluster) nehmen oder nicht.

Einmal Vorname, einmal
Dann einmal Nachname
 
Hier fehlt ein Punkt beim K


![Screenshot from 2021-12-03 09-50-32](https://user-images.githubusercontent.com/90834735/144599290-e0435f4a-fa84-498b-9eb0-0086b1555982.png)





**Clustervorschläge**
9 mal kommt es vor, dann ist das andere ein Tippfehler, das nehmen wo häufiger vorkommt
Die ausgewählten werden neu berechnet, kann aber sein dass noch mehr cluster entstehen
Nearest neighbor wieviele schritte, dass Abweichung kommt (anderer Clustervorschlag).

**Datensätze wieder zusammenführen**
In Zeilen arbeiten, nur bei bedarf aufsplitten.
Gibt noch eine Record Ansicht.
Jetzt werden die Autoren, wo vorher gesplitten waren, wieder zusammengefügt die Autoren, man soll sich Zahl 1001 merken von vorher, jetzt auch wieder 1001 sind
 
**einfache Aufgaben:**
Auf Eddit gehen und das Leerzeichen rausnehmen, dann apply.
 
Jetzt erscheint nur einmal
![MDPI](https://user-images.githubusercontent.com/90834735/150561990-86b70bd4-5d9b-4d2c-9f18-0a89c4effe17.png)


**MDPI AG hat Leerzeichen drin, dann erscheint zweimal, muss also rausnehmen.**
Cluster hätte man nutzen können.



**AHA-Moment!**

Wikidata gibt es Einträge in der Datenbanken für Zeitschriften.
Wir haben dann eine als Beispiel genommen und diese mit der open refine die Daten zu vervollständigen.
![Screenshot from 2021-12-03 10-58-01](https://user-images.githubusercontent.com/90834735/144600421-efadb36a-584e-4217-b64d-f538b732c0e0.png)


Abgleich machen zu können, brauchen wir einen Namen. 
Neue spalte anlegen, basierend auf neuer spalte (excel ist einfacher)
 
	 
Kann if und for- Schleifen, kann so komplexe Transformationen machen. Könnte das im libary carpentry lernen.

Im wiki  kann mit isbn 10 auf isbn 13 kann das mit copy paste einfach anwenden.
Grell Sprache um nur den Zeitschriftenname zu ändern.
Split funktion wird wieder gebraucht.

Value ist Standarwert , übernimmt aus der voherigen Spalte
value.split(",")[0]
Value. Split («,»)   ()= Funktion  ,aber Eckige Klammer beginnt bei 0, wollen aus dem Ergebnis nur ersten Teil
 

 ![grell](https://user-images.githubusercontent.com/90834735/150562105-8e4520e8-8219-4acd-8442-4688e4342386.png)


	 
Oder ich kann auch klicken, wenn nicht mit der Programmiersprache **grell** eine Funktion schreiben möchte.
 
**AHA-Moment:* Neue Spalte **Journal** ist erschienen und wurde ausgefüllt anhand der Daten von wikidata, es hat die Daten zum Journal von wikidata genommen und gerade eingefügt:

![Screenshot from 2021-12-03 10-41-55](https://user-images.githubusercontent.com/90834735/144601613-11a4163e-ad96-4c00-956a-49760a96f818.png)





**Reconcilation (Fachbegrifff im open refine für den Abgleich mit externe Daten  wie z.b. aus dem wikidata)**
Journal klicken, dann Start Reconciliation.
 
 ![Screenshot from 2021-12-03 11-05-57](https://user-images.githubusercontent.com/90834735/144601083-03099dcb-3abd-4dc1-b712-fb66785cc50f.png)
 
 
**Reconilation mit Wikidata**	 
Man könnte in fragekommende Datensätze auf diesen Typ einschränkt, aber es können auch Vorschläge verloren gehen, jetzt im wikidata ein paar
Datensätze anschauen, denn nicht alle Datensäzte sind diesen journals eingeordet,
dann soll man lieber allgemeine kategorie drin lassen.
Kein bestimmter typ
Mensch, dass nicht ortschaft kommt?
 
Start reconciling
Sind 1001 Datensätze aber nicht viele werte, 51 verschiedene werte
Im Hintergrund wird jetzt ein Dienst angefragt, und open refine ermittelt jetzt für spalte Journal aus den datensätzen von wikidata.
www.wikidata.org
	 
 
Offical webseite
Gibt noch 2. Issn für printausgabe
 ![Screenshot from 2021-12-03 10-55-27](https://user-images.githubusercontent.com/90834735/144600301-307cc8a9-78e9-4a5a-ade5-f7248688861e.png)

  
	 
Bessere Vorschläge, kann nicht nur mit Zeitschriftennamen kann auch mit Personennamen und Geburtsdatum im wikidata abgleichen.
 
Jetzt gibt’s die Match aus dem Wikidata für bestimmte Zeitschriften .
Nicht überall. 952 Zeitschriften wurden gematcht mit wikidata.
 
 
Mache **all identical cells**
![entropy](https://user-images.githubusercontent.com/90834735/150562221-157d05ba-298c-4b00-8296-2c2ca1c6fca7.png)




**AHA-Moment:  Bei der Spalte Journal und dann gehe zu Entropy: Nun kann ich alle Matches von wikidata und open refine zuordnen und sehen!**
	
	 

 
	
 
 
**Scopus ID** Scopus bezeichnet sich als grösste Datenbank mit  Abstract and Zitationen, diese sind  aus peer-reviewed (wissenschaftlich geprüfter) Literatur: Wissenschaftliche Zeitschriften, Bücher und Konferenz und Tagungsbeiträge. Ich kann auch nach einer bestimmten ID suchen, nehmen wir eben die Scopus.
ich habe das Urban Agriculutre Journal Basel genommen und dort auf Scopus ID gesucht, der Abgleich erscheint jetzt. Eigentlich wollte ich nicht nur Scopus auf nur ein Journal suchen sondern auf alle.
![Screenshot from 2021-12-03 11-08-39](https://user-images.githubusercontent.com/90834735/144603402-c75d590b-d886-45ff-b1e1-83513c106896.png)


	
 

**Man könnte bei Personennamen mit der Datenbank von Viaf abgleichen**
 
	

**CSV nach MARC XML : Modellieren im marc xml exportieren**
Wir wollen jetzt marc xml produzieren, vorher war es in json, das heisst die Voreinstellung ist in json. Wenn schliesse geht alles verloren.

![Screenshot from 2021-12-03 11-28-04](https://user-images.githubusercontent.com/90834735/144596779-bde681ec-ea1b-48bf-ab94-87d11691384f.png)

**Marc xml**  
Das ist Struktur wie wir machen müssten.Wenn man so ein Template schreibt, könnte man für alle 1000 Datensätze nehmen.
Aber wir können nur für Autor oder Titel machen.Namespaces hat er marc doppelpunkt entfernt.
 
Template geht für alle Datensätze, wenn zwei **geschweiffte Klammern stehen, da kommen da die Ihalte rein**, cells **url** also der Inhalt, der Spalte, alte url wird dann übernommen und **https doa.org/artickle wird ersetzt** nur identifier bleibt stehen über noch ein Punkt escape dass alles xml entspricht. Weil xml erlaubt **keine Umlaute, diese müssen entfernt werden**

Gibt auch eine for- schleife- für mehrer Autoren.




**reverse engineering**
erste Feld für 001, ist schon geschireben
Feld issn, die Umlaute wegnehmen aber Autoren.
Split 0 funktion was die mit Autor macht,
erste wert wird nur genommen, es wird also flavia genommen und das wird in dieses feld 100a geschreiben und dnan wieder ins xml kopiert.
Autor 700 also diese slice schneidet den wert weg, diese wird dann in wert ins subfield code a genommen.
 
	 
**Übungen Template ergänzen**
Wir wollen Marc Felder füllen, Bis jetzt haben wir autor, titel, url, issn genommen
dazu musste man auch wieder die beispieldatenbank aufrufen. die fand ich dann später bei den open projects.
m
![template](https://user-images.githubusercontent.com/90834735/150562308-7f308e7b-159f-49d9-b184-e78730c5709e.png)

 


Ich habe auch das Feld mit der Doi gefunden. Diese Datei habe ich dann wieder abgespeichert, aber noch nicht validiert. Ich muss die übung vielliecht nochmals machen, diese wurde nur kurz vorgezeigt. Diese wurde auch wieder im export templeting gemacht.

Jetzt noch die doi nehmen, Befehle waren so:
{{
forNonBlank(
    cells['DOI'].value,
    v,
    '<datafield tag="024" ind1="7" ind2=" ">
        <subfield code="a">' + v.escape('xml') + '</subfield>
        <subfield code="2">doi</subfield>
    </datafield>',
    ''
)
}}

 
Zuerst muss mal Doi Spalte anschauen, 73  wo es keine Doi hat muss man in geschweiffte Klammern um abzugrenzen.
Gibt nur null, dann müsste jetzt eine if abfrage, wenn das Feld leer, dann mach das.
Oder man kann Nonblade machen, das heisst wenns blank (leer) ist, nichts reinschreiben.
 
 - Wenns blank ist, dann nicht, und sonst datafield
 - Wenn Datensätze leer sind, dann nichts und sonst doi.
 - Wenn das Feld nicht leer ist, dann doi schreiben in feld 024? Unterfeld a2.

das waren die Befehle:


 ***nun ging es wieder zurück zur angefangen Template** zum Glück standen die Befehle noch richtig drin,die ich vorher copy pastet habe. dann musste man auf ausführen (execute drücken) und es hat einem ein txt. datei gemacht. 
 
  es hat mir wieder ein doaj-article als txt.Datei gemacht. 
  ![doaj articel](https://user-images.githubusercontent.com/90834735/150564954-908630d1-81cf-4f80-bb6c-9f8ff579ab00.png)

 
 
Diese war dann verfügbar, zuerst konnte ich sie nicht speichern. habe dann aber noch Hilfe bekommen von den Dozenten (vielen Dank!) und diese in den downloads gespeichert, nun galt es diese Datei in xml umzubennen und danach zu speichern.
**AHA-Moment**: Man hätte separaten Ordner machen sollen, weil bei der späteren Validierung hat es dann einfach alle Dokumente genommen, und diese validiert.
Ein Dokument war ja von Archive space in Ead- Format, das gibt’s im marc21 nicht dann gibt es eine Fehlermeldung in der SHELL Validierung.
	
**AHA-Moment** So haben wir schon fast gültige Datensätze, diese müsste man noch validieren mit xmilini. Xmilini ist ein Programm, das auf UBUNTU (Shell) schon vorinstalliert war. Die Datensätze haben wir dann Befehlen, die wir copy/pasten konnten, validiert.
**Es hat also den Standard das Schema Marc21 aus dem loc.gov mit der doaj-articel-sample verglichen.**

cd ~/Downloads
wget https://www.loc.gov/standards/marcxml/schema/MARC21slim.xsd
xmllint doaj-article-sample-csv.xml --noout --schema MARC21slim.xsd

**AHA-Moment: So kann man prüfen ob es dem Schema marc21 entspricht, hatten wir im gset sonst immer mit einem Validierungs-tool im Browser gemacht, das war neu für mich und dann erfolgt die Validierung.**
![Screenshot from 2021-12-03 13-53-45](https://user-images.githubusercontent.com/90834735/144605936-041a11bf-165c-4a88-beae-dcae28f4d20f.png)


 

Die Validierung ist wichtig auch in Nationalbibliotheken, dort wird geprüft ob die Reihenfolge und Zahlen und keine Buchstaben und ob das alles passt. Weil wenn man Daten manuell eingibt, gibt viele Fehler.
 
 **AHA-Moment!**
Validierung in der SHELL mit diesen Befehlen:
 
**Validierung in der SHELL /TERMINAL**
![Screenshot from 2021-12-03 12-19-13](https://user-images.githubusercontent.com/90834735/144594143-1f1a6cb5-4490-4a95-965a-b50adf2d943d.png)


Dann habe es validiert, da hat es dann aber alle Dokumente genommen,  aber man kann sehen, dass der doaj-article-sample-csv.xml validates geklappt hat das validieren, bei den anderen downloads die auch noch drin waren, hat die Validierug nicht geklappt.

Aber das war ja auch nicht das Ziel. Ziel war es ja das erstellte Template, dass nun xml konform zu sein scheint, auf seine Gültigkeit zu prüfen, ich wusste gar nicht, dass man das im Terminal auch kann mit einem Befehl. In der Schule im GSET Unterricht haben wir jeweils mit einem Tool im Browser (https://onlinexmltools.com/)  validiert.

Open refine speichert alles, auch wenn beendet habe.


 **AHA-Moment**:In meinem Ordner "Downloads" hat es noch andere Dokumente drin, diese wurden halt auch mit validiert...
 
 So das war viel Info. Tschau liebes Tagebuch!
 
