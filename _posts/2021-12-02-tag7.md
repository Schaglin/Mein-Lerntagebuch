---
title: "Tag 7"
date: 2021-12-02
---
_Liebes Tagebuch_,

**Transformation von Metadaten mit Open Refine**

Im Naturkunde Museum, war ein Dozent am arbeiten. Die Sammlung war da noch in Exceltabellen. Jetzt wollen sie das gerne zusammenführen, wie kriegt man diese unstrukturierten Daten aus dem Excel in ein Format wie dublin core oder marc21?
Die Lösung heisst OpenRefine!

**OpenRefine** ist unabhängig von Koha etc. und wird nicht nur in Bibliothek oder Archiven angewendet.
Open Refine beschreibt sich als **_a free open source, powerful tool for working with messy data_**
Bei der Erschliessung passieren auch Fachmitarbeiter Tippfehler etc., die Daten sind zwar besser als Amazon aber passieren doch Fehler. Open Refine ist interessant, die nicht gerne mit Terminal arbeiten wollen.Aber doch sich zutrauen, einzelne kleine Scripte zu schreiben. z. B. Die sollen von Koha in Alma gehen und wenn jetzt der Koha Export nicht perfekt, ist, kann es mit open refine gemacht werden. **Understanding data you don’t know**. Also ein Museum, Archive gibt uns Daten rüber, dann Hr. Lohmeier erste Schritt geht ins open refine, wenn steht Marc21, kann man nicht darauf vertrauen, dass es auch Marc21 ist.

**Reconciliation**: Nutzung externe Datenquelle, um eigene Daten anzureichern.
Abgleich mit Normdaten in Wikidata, GND und VIAF.

**Bibliotheken**: Marc Edit ist zwar naheliegender für Marc21, aber open refine geht auch, bevor sie dann ins Bibliotheksystem integriert werden.
Data Sience, Semantic Web, Consulting,Business , Research, aber am meisten doch Bibliotheken. Aber auch gutes Zeichen, wenn diese Software von Data Science arbeiten.

**Welche Formate gehen gut mit Open Refine?**
Von Open Refine unterstützte Daten: Besonders gut mit Tabellendaten
(CSV, TSV, XLS, XLSX auch TXT)
XML geht auch wenn ein flaches ist wie MARCXML

**Json Apis**:Normalerweise Nutzung von JSON-APIs, wenn nicht im Archiv oder Bibliothekswesen. JSON ist neuer als Marc XML mit OAI-PMH Schnittstelle.Ich muss zuerst mal googlen, was jetzt mit JSON-API genau gemeint ist:[json-api.org] (www.jsonapi.org/) https://jsonapi.org/

XML sei komplex wie EAD ist schwieriger mit Zusatztool   (Hr.Lohmeier für Marbach literaturarchiv gemacht)
Kann auch mit Marc Edit, weil hat dort Exportfunktion für Marc Daten für open refine, dann kann man sie besser in open refine transformieren, dann wieder in Marc Edit zurückmachen.


**Einsatzmöglichkeiten**:126 Mio von Nationaldatenbibliotheken hat Peter Kiraly verglichen, da hat es Fehlerquoten von 20 % ,der den Marc21 Datensätzen widerspricht.
Schlimme Fehler z.b. ist falsche Marc Feld verwendet worden Titel statt Autor. Darum brauchts einen Abgleich mit Normdaten (VIAF, GND, wikidata).

 **Installation open refine**
open refine wurde  installiert konnten aufrufen auf (http://localhost:3333/)
Das klappte auf Anhieb! :-)

**Vorstellung open refine und  einfache Funktionen wurden erklärt:**
Open refine hat erkannt, dass es csv data sind.
Beliebiges Trennzeichen oder Tabs oder Kommas.
Beim Import soll man aber möglichst wenig tun, weil es Automatismen hat. Weil man wird in Änderungshistorie sehen.
Die ersten 100 Datensätze werden mal angezeigt, da soll man schauen, ob alles Sonderzeichen é und so auch übernommen wurden und richtig dargestellt sind, sonst ein Problem. Aber meisten Datensätze sind in unicode oder utf8.
 
**Create Project**
Wenn Standard utf8 nicht funktioniert, müsste man hier anpassen. Dann auf **create project**, dann kommt Hauptbildschirm (Projekte) kann nun arbeiten mit den Daten.Ansicht erinnert schon an Excel, aber nur ersten 10 Datensätze sieht man, anpassen, dass man 50 Zeilen sehen kann.
Vorteil on open refine ist  eben grosse Datenmengen anzeigen kann, Excel ist da schwieriger im Excel gab es früher 246 Spalten und 65'000 Zeilen (bis Jahr 2013).

**Aufgaben wurden gemacht*:
![EN](https://user-images.githubusercontent.com/90834735/150561878-eceaaa77-9387-4552-a32d-cbd74bebbc12.png)


**undo/redo**
Es gibt eine Speicherung des Verlaufes: undo/redo. Wenn ein Fehler passiert, kann einfach redo machen.

**Clusterig und Algorithmen, die Namensvorschläge bringen im open refine**:

9 mal kommt es vor, dann ist das andere ein Tippfehler, das nehmen wo häufiger vorkommt.
![nearest neighbor](https://user-images.githubusercontent.com/90834735/151655052-b58dff2c-800e-4498-ac9e-4966cb076fd9.png)

**Datensätze wieder zusammenführen**

Auf Eddit gehen und das Leerzeichen rausnehmen, dann apply.
 
![MDPI](https://user-images.githubusercontent.com/90834735/150561990-86b70bd4-5d9b-4d2c-9f18-0a89c4effe17.png)


![journal](https://user-images.githubusercontent.com/90834735/151655132-ea341d86-9756-4bc2-9037-5b90fb791832.png)

 ![grell](https://user-images.githubusercontent.com/90834735/150562105-8e4520e8-8219-4acd-8442-4688e4342386.png)
 

 Neue Spalte **Journal** ist erschienen und wurde ausgefüllt anhand der Daten von wikidata, es hat die Daten zum Journal von wikidata genommen und gerade eingefügt:

![Screenshot from 2021-12-03 10-41-55](https://user-images.githubusercontent.com/90834735/144601613-11a4163e-ad96-4c00-956a-49760a96f818.png)


![wikidata](https://user-images.githubusercontent.com/90834735/151655298-243ccc74-49d4-4c0c-9f17-107b201c76ea.png)

**Reconciliation mit einem Journal aus wikidata)**
Journal klicken, dann Start Reconciliation.
 
 ![Screenshot from 2021-12-03 11-05-57](https://user-images.githubusercontent.com/90834735/144601083-03099dcb-3abd-4dc1-b712-fb66785cc50f.png)
 
Im Hintergrund wird jetzt ein Dienst angefragt, und open refine ermittelt jetzt für Spalte Journal aus den Datensätzen von[wikidata] (www.wikidata.org)
**WOW!**:952 Zeitschriften wurden gematcht mit wikidata!
 
 
Mache **all identical cells**
![entropy](https://user-images.githubusercontent.com/90834735/150562221-157d05ba-298c-4b00-8296-2c2ca1c6fca7.png)


**AHA:  Bei der Spalte Journal und dann gehe zu Entropy: Nun kann ich alle Matches von wikidata und open refine zuordnen und sehen!**
	

 
**Scopus ID** Scopus bezeichnet sich als grösste Datenbank mit  Abstract and Zitationen, diese sind  aus peer-reviewed (wissenschaftlich geprüfter) Literatur.
![agriculture](https://user-images.githubusercontent.com/90834735/151655285-5e260993-2a2c-4c93-bf12-dd4b21207fd8.png)


**Man könnte bei Personennamen mit der Datenbank von Viaf abgleichen**
 
**CSV nach MARC XML : Modellieren im marc xml exportieren**
Wir wollen jetzt marc xml produzieren, vorher war es in json, das heisst die Voreinstellung ist in json. Wenn schliesse geht alles verloren.

**Marc xml**  
Das ist Struktur wie wir machen müssten.Wenn man so ein Template schreibt, könnte man für alle 1000 Datensätze nehmen.
Aber wir können nur für Autor oder Titel machen.Namespaces hat er marc doppelpunkt entfernt. Template geht für alle Datensätze, wenn zwei **geschweiffte Klammern stehen, da kommen da die Inhalte rein**, cells **url** also der Inhalt, der Spalte, alte url wird dann übernommen und **https doa.org/artickle wird ersetzt** nur identifier bleibt stehen über noch ein Punkt escape dass alles xml entspricht. Weil xml erlaubt **keine Umlaute, diese müssen entfernt werden** Gibt auch eine for- schleife- für mehrer Autoren.

**Template ergänzen**
Wir wollen Marc Felder füllen, Bis jetzt haben wir autor, titel, url, issn genommen
dazu musste man auch wieder die beispieldatenbank aufrufen. die fand ich dann später bei den open projects.

![template](https://user-images.githubusercontent.com/90834735/150562308-7f308e7b-159f-49d9-b184-e78730c5709e.png)
**mit Grell Sprache DOI hinzufügen** 
Zuerst muss mal Doi Spalte anschauen,  wo es keine Doi hat muss man in geschweiffte Klammern um abzugrenzen.
Gibt nur null, dann müsste jetzt eine if abfrage, wenn das Feld leer, dann mach das.
Oder man kann Nonblade machen, das heisst wenns blank (leer) ist, nichts reinschreiben.
  _Beispiel mit der DOI_ wurde gezeigt.
** Export Templing**: Ich habe auch das Feld mit der Doi gefunden. Diese Datei habe ich dann wieder abgespeichert, aber noch nicht validiert. 
Jetzt noch die doi nehmen, Befehle waren so:
-{{
-forNonBlank(
   - cells['DOI'].value,
   - v,
    -'<datafield tag="024" ind1="7" ind2=" ">
       - <subfield code="a">' + v.escape('xml') + '</subfield>
       - <subfield code="2">doi</subfield>
   - </datafield>',
    -''
-)
-}}





 ***nun ging es wieder zurück zur angefangen Template** zum Glück standen die Befehle noch richtig drin,die ich vorher copy pastet habe. dann musste man auf ausführen (execute drücken) und es hat einem ein txt. datei gemacht. 
 
  es hat mir wieder ein doaj-article als txt.Datei gemacht. 
  ![doaj articel](https://user-images.githubusercontent.com/90834735/150564954-908630d1-81cf-4f80-bb6c-9f8ff579ab00.png)

Diese war dann verfügbar, zuerst konnte ich sie nicht speichern. habe dann aber noch Hilfe bekommen von Dozent Hr. Meyer (vielen Dank!) und diese in meinen Downloads gespeichert, nun galt es diese Datei in xml umzubennen und danach zu speichern.
*
	
**Validieren** So haben wir schon fast gültige Datensätze, diese müsste man noch validieren mit xmilini. Xmilini ist ein Programm, das auf UBUNTU (Shell) schon vorinstalliert war. Die Datensätze haben wir dann Befehlen, die wir copy/pasten konnten, validiert.
**Es hat also den Standard das Schema Marc21 aus dem loc.gov mit der doaj-articel-sample verglichen.**

cd ~/Downloads
wget https://www.loc.gov/standards/marcxml/schema/MARC21slim.xsd
xmllint doaj-article-sample-csv.xml --noout --schema MARC21slim.xsd

**AHA-Moment: Validieren kann man auch mit Shell-Befehlen!.**
![Screenshot from 2021-12-03 13-53-45](https://user-images.githubusercontent.com/90834735/144605936-041a11bf-165c-4a88-beae-dcae28f4d20f.png)
Die Validierung hatten wir im Modul "Gset" sonst immer mit einem Validierungs-Tool(https://onlinexmltools.com/)  validiert im Browser gemacht, das war neu für mich.

![Screenshot from 2021-12-03 12-19-13](https://user-images.githubusercontent.com/90834735/144594143-1f1a6cb5-4490-4a95-965a-b50adf2d943d.png)

Dann habe es validiert, da hat es dann aber alle Dokumente genommen,  aber man kann sehen, dass der doaj-article-sample-csv.xml validates geklappt hat das validieren, bei den anderen downloads die auch noch drin waren, hat die Validierug nicht geklappt.
Open refine speichert alles, auch wenn es beendet wurde.

 **AHA-Moment**:In meinem Ordner "Downloads" hat es noch andere Dokumente drin, diese wurden halt auch mit validiert.
 
Die **Validierung** sei wichtig auch in Nationalbibliotheken, dort wird geprüft ob die Reihenfolge und Zahlen und keine Buchstaben und ob das alles passt. Weil wenn man Daten manuell eingibt, gibt viele Fehler.

Ich muss aufhören, schon wieder die 4'000 Zeichen überschritten.Tschau liebes Tagebuch!
 
