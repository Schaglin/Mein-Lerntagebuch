**Tag 7 Transformation von Metadaten mit Open Refine**

Metadata von Beginn an zu zu schaffen
•	Transformation (Veränderung) von Metadaten mit OpenRefine
•	Weitere Tools zur Metadatentransformation


Normalerweise Nutzung von JSON-APIs   wenn nicht im Archiv oder Bibliothek


Metadata-Modellieren: Mapping  sind ja diese Regeln, um ein Format in ein anders zu übertragen

Naturkunde Museum, war der Hr. Lohmeier. Sammlung noch in Exceltabellen, solange begrenzte Anzahl von Datensätzen. Jetzt wollen sie das gerne zusammenführen, wie kriege ich jetzt diese unstrukturierten daten in dublin core oder marc21?

OpenRefine unabhängig von  Koha etc. nicht nur in Bibliothek oder Archiv.
Claim: a free open source, powerful tool for working with messy data.
Hat Bibliothek keine messy data, aber doch auch Tippfehler hat es drin, auch trotzt Regelwerk sind nicht fehlerfrei. Bei der Erschliessung sind die Daten vielleicht besser als Amazon aber passieren doch Fehler.Ein mächtiges Tool.
Das besondere an Open Refine ein Mischling aus Tabellenverarbeitungssoftware.
Open Refine ist interessant, die nicht gerne mit Terminal arbeiten wollen.Aber doch sich zutrauen, einzelne kleine Scripte zu schreiben. Sieht aus wie Excel. Man kann Fehler bereinigen, in versch. Daten exportieren und auch externe Datenquellen heranziehen.
Schnittstelle zur gemeinsamen Normdatei. Wenn Geburtsort und Sterbeort noch ergänzen.
Wird in der Regel lokal auf Computer installiert, wird aber über Browser bedient.
Es ist aber keine Cloud-Software. Sind denn meine Daten sicher? Ja, als ob ich kleinen Webserver auf meinem Computer installiere.
Es gehen keine Daten nach draussen, ausser ich setzte externe Schnittstellen ein.

 
Also am meisten wird es eingesetzt zur Bereinigung von Tippfehlern.
z. B. Die sollen von Koha in Alma gehen und wenn jetzt der Koha Export nicht perfekt, ist, kann es mit open refine gemacht werden.
Understanding data you don’t know. Museum, Archive gibt uns Daten rüber, dann Hr. Lohmeier erste Schritt geht ins open refine, wenn steht Marc21, kann man nicht darauf vertrauen, dass es auch Marc21 ist.
Reconciliation: Nutzung externe Datenquelle, um eigene Daten anzureichern.


 Bibliotheken : Marc Edit ist zwar naheliegender für Marc21, aber open refine geht auch, bevor sie dann ins Bibliotheksystem integriert werden.
Data Sience, Semantic Web, Consulting,Business , Research, aber am moisten doch Bibliotheken. Aber auch gutes Zeichen, wenn diese Software von Data Science arbeiten.

Von Open Refine unterstützte Daten: Besonders gut mit Tabellendaten
(CSV, TSV, XLS, XLSX auch TXT)
XML geht auch wenn ein flaches ist wie MARCXML
JSOn ist mit etwas Übung einfach zu modellieren
XML kompex wie EAD ist schwieriger mit Zusatztool   (Lohmeier für Marbach literaturarchiv gemacht)
Kann auch mit Marc Edit, weil hat dort Exportfunktion für Marc Daten für open refine, dann kann man sie besser in open refine transformieren, dann wieder in Marc Edit zurückmachen.
Open refine ist gut für Massendaten, darum hat Terry Reese dieses Tool auch eingesetzt

**Einsatzmöglichkeiten**
Vereinheitlichung und Bereinigung     
126 Mio von Nationaldatenbibliotheken hat Peter Kiraly verglichen, da hat es Fehlerquoten von 20 % ,der den Marc21? Datensätzen widerspricht.
Verlagsdaten wo kein geschultes Personal, dann noch migriert, dann hats Fehler zusammen.
Schlimme Fehler z.b. ist falsche Marc Feld verwendet worden Titel statt Autor
Abgleich mit Normdaten (Renconciliation) in Wikidata, GND und VIAF
Für lokalen Einsatz ausgelegt (Installation auf Webserver, Zusatzsoftware)
 
Grosse Entwicklercommunity, auf github
Von Firma metaweb, ist von google aufgekauft worden google refine
Google keine Verwendung mehr, dann wurde es open source
98 verschiedene Menschen, haben zu open refine beigetragen (Auch Übersetzungen)
 
Vufind hat nur 212 Sterne

5min hat er auto Save
 
Get und post der redet mit dem browser
Wenn open refine ausgeschaltet ist, kann ich logischerweise auch im browser bedienen
 
**Vorstellung open refine und  einfache Funktionen wurden erklärt:**
Open refine hat erkannt, dass es csv data sind
Beliebiges trennzeichen geht auch
Oder tabs oder commas
Bei import soll man aber möglichst wenig tun (automatismen) Weil wird in änderungshistorie sehen
Aber leerzeichen in tabellen hat oder spalten gehen über 2 zeilen, dann kann man das einstellen
Die ersten 100 Datensätze werden mal angezeigt, da soll man schauen, ob alles Sonderzeichen é und so auch übernommen wurden und richtig dargestellt sind. Wenn merkwürdiges Fragezeichen steht, dann gäbe es ein Problem. Aber meisten Datensätze sind in unicode oder  utf8
 
![Screenshot from 2021-12-03 09-19-27](https://user-images.githubusercontent.com/90834735/144598224-cc2af6b4-5908-4e61-a1d1-377e611a155c.png)

Wenn standard utf8 nicht funktioniert, müsste man hier anpassen
Dann auf **create project**, dann kommt hauptbildschirm (projekte) kann nun arbeiten mit den Daten
Ansicht erinnert schon an Excel, aber nur ersten 10 Datensätze sieht man
Anzahl zeilen möchte ersten 50 sehen
 

Vorteil eben grosse Datenmengen, Excel da schwieriger Im Excel gab es früher gabe es nur 246 Spalten und 65'000 Zeilen, gilt zwar heute nicht mehr, war aber noch 2013 der Fall
 
Für änderungshistorie
 

**AHA-Moment: EN und Englisch ist ja ungleich, wo hat es mehr bei EN hat es mehr, also kann ganz einfach im Edit ändern. einmal leer, 871 xEN, 107x English
![Screenshot from 2021-12-03 09-31-12](https://user-images.githubusercontent.com/90834735/144598525-b0c1a4b0-aef4-4355-9286-507470eb0e7f.png)
 
dann sehe ich 107 solcher Englisch in EN sind nun gändert worden von Englisch zu EN, 

**undo/redo**
Es gibt eine Speicherung des Verlaufes: undo/redo
sehe im undo/redo
Transformation da  wegen den änderungen
Achtung, dass nicht facette aufhabe
 
7 sieht man hier, die spanischen Autoren /Zeitschriften
 
	Achtung, dass nicht noch ein Filter oder Facette aktiv habe, ich soll immer als Orientierung schauen im Titel, dass nicht noch eine Facette aktiv war.

Kann auch ein Sprache exkludieren französich und englisch
 
 
	Yoshinowo als einzelautor kam 7 x vor, aber man weiss nicht, ob in mehrfach solche autoren, noch häufiger waren: ACHTUNG vor 
 
	Mehrfachbelegung
 


 
3000 können wir bestätigen
Dann bekommt man die Autorennamen
 
 
	

**Clusterig und Algorithmen, die Namensvorschläge bringen im open refine**:
	Hier kann Leerzeichen einfach entfernen
Mal werden Namen abgekürzt, mal nicht, gibt Cluster Button dafür
 
	Gibt Vorschläge vom Computer wo Ähnlichkeiten sind, hat Algorithmus gemacht, jetzt kann ich diese Vorschläge (Cluster) nehmen oder nicht.
	
Einmal Vorname, einmal
Dann einmal Nachname
 
Hier fehlt ein Punkt beim K


![Screenshot from 2021-12-03 09-50-32](https://user-images.githubusercontent.com/90834735/144599290-e0435f4a-fa84-498b-9eb0-0086b1555982.png)





9 mal kommt es vor, dann ist das andere ein Tippfehler, das nehmen wo häufiger vorkommt
 
	 
	 
	Die ausgewählten werden neu berechnet, kann aber sein dass noch mehr cluster entstehen
Nearest neighbor wieviele schritte, dass abweichung kommt (anderer Clustervorschlag)
 
	

Datensätze wieder zusammenführen
In zeilen arbeiten, nur bei bedarf aufsplitten
Gibt noch record ansicht
 
 
	Jetzt werden die Autoren, wo vorher gesplitten waren, wieder zusammengefügt die Autoren, man soll sich Zahl 1001 merken von vorher, jetzt auch wieder 1001 sind
 
**einfache übungen:**
Auf Eddit gehen und das Leerzeichen rausnehmen, dann apply
 
Jetzt erscheint nur einmal
![Screenshot from 2021-12-03 10-23-42](https://user-images.githubusercontent.com/90834735/144599352-3f71fd85-c060-43e6-9369-6b4a25e63fc6.png)

MDPI AG hat Leerzeichen drin, dann erscheint zweimal, muss also rausnehmen
 	
 
Cluster hätte man nutzen können.



**AHA-Moment!**

Wikidata gibt es einträge in der datenbanken für zeitschriften
wir haben dann eine als Beispiel genommen und diese mit der open refine die daten zu vervollständigen.
![Screenshot from 2021-12-03 10-58-01](https://user-images.githubusercontent.com/90834735/144600421-efadb36a-584e-4217-b64d-f538b732c0e0.png)


Abgleich machen zu können, brauchen wir einen namen
 
Neue spalte anlegen, basierend auf neuer spalte (excel ist einfacher)
 
	 
Kann if und for schleifen, kann so komplexe transformationen machen
Könnte im libary carpentry lernen

Im wiki  kann mit isbn 10 auf isbn 13 kann das mit copy paste einfach anwenden
Grell sprache um nur den zeitschriftenname zu ändern
Split funktion wird wieder gebraucht
Value ist Standarwert , übernimmt aus der voherigen Spalte
value.split(",")[0]
Value. Split («,»)   ()= Funktion  ,aber Eckige Klammer beginnt bei 0, wollen aus dem Ergebnis nur ersten Teil
 
![Screenshot from 2021-12-03 10-38-13](https://user-images.githubusercontent.com/90834735/144599509-d15d49de-f5c3-4603-b73a-43136f53bad4.png)
 

	 
Oder ich kann auch klicken, wenn nicht mit der Programmiersprache **grell** eine funktion schreiben möchte
 
**AHA-Moment:* Neue Spalte **Journal** ist erschienen und wurde ausgefüllt anhand der daten von wikidata, es hat die daten zum journal von wikidata genommen und gerade eingefügt:

![Screenshot from 2021-12-03 10-41-55](https://user-images.githubusercontent.com/90834735/144601613-11a4163e-ad96-4c00-956a-49760a96f818.png)





**Reconcilation (Fachbegrifff im open refine für den Abgleich mit externe Daten  wie z.b. aus dem wikidata)**
Journal klicken, dann Start Reconciliation
 
 ![Screenshot from 2021-12-03 11-05-57](https://user-images.githubusercontent.com/90834735/144601083-03099dcb-3abd-4dc1-b712-fb66785cc50f.png)
 
 
	 
	Man könnte in fragekommende datensätze auf diesen typ einschränkt, aber es können auch vorschläge verloren gehen, jetzt im wikidata ein paar
Datensätze anschauen, denn nicht alle datensäzte sind diesen journals eingeordet,
dann soll man lieber allgemeine kategorie drin lassen.
 
	Kein bestimmter typ

Mensch, dass nicht ortschaft kommt?
 
 
	Start reconciling
Sind 1001 Datensätze aber nicht viele werte, 51 verschiedene werte
Im Hintergrund wird jetzt ein Dienst angefragt, und open refine ermittelt jetzt für spalte Journal aus den datensätzen von wikidata.
www.wikidata.org
 
	 
 
Offical webseite
Gibt noch 2. Issn für printausgabe
 ![Screenshot from 2021-12-03 10-55-27](https://user-images.githubusercontent.com/90834735/144600301-307cc8a9-78e9-4a5a-ade5-f7248688861e.png)

 
	 
	 
Bessere Vorschläge, kann nicht nur mit Zeitschriftennamen kann auch mit Personennamen und geburtsdatum im wikidata abgleichen
 
Jetzt gibt’s die Match aus dem Wikidata für bestimmte Zeitschriften 
Nicht überall
 
952 wurden gematcht mit wikidata
 
 
Mache **all identical cells**

![Screenshot from 2021-12-03 10-53-48](https://user-images.githubusercontent.com/90834735/144602043-9bd94a82-b488-41e8-8646-a6f0748008b8.png)


 
	AHA-Moment:  Bei der Spalte Journal und dann gehe zu Entropy: Nun kann ich alle Matches von wikidata und open refine zuordnen und sehen!
	
	 

 
 
 
	
 
 
**Scopus ID** Scopus bezeichnet sich als grösste Datenbank mit  Abstract and Zitationen, diese sind  aus peer-reviewed (wissenschaftlich geprüfter)  Literatur: Wissenschaftliche Zeitschriften, Bücher und Konferenz und Tagungsbeiträge.	ich kann auch nach einer bestimmten ID suchen, nehmen wir eben die Scopus.
ich habe das Urban Agriculutre Journal Basel genommen und dort auf Scopus ID gesucht, der Abgleich erscheint jetzt. Eigentlich wollte ich nicht nur Scopus auf nur ein Journal suchen sondern auf alle.
![Screenshot from 2021-12-03 11-08-39](https://user-images.githubusercontent.com/90834735/144603402-c75d590b-d886-45ff-b1e1-83513c106896.png)


	
 
	

**Man könnte bei Personennamen mit der Datenbank von Viaf abgleichen**
 




	

**CSV nach MARC XML : Modellieren im marc xml exportieren**
Wir wollen jetzt marc xml produzieren, vorher war es in json, das heisst die voreinstellung ist in json. Wenn schliesse geht alles verloren.

![Screenshot from 2021-12-03 11-28-04](https://user-images.githubusercontent.com/90834735/144596779-bde681ec-ea1b-48bf-ab94-87d11691384f.png)

 
	Marc xml  
	 
	Das ist struktur wie wir machen müssten
Wenn man so ein template schreibt, könnte man für alle 1000 nehemn
Aber wir können nur für autor oder titel machen
Namespaces hat er marc doppelpunkt entfernt
 
Template geht für alle Datensätze, wenn zwei **geschweiffte klammern stehen, da kommen da die inhalte rein**, cells **url** also der inhalt der spalte url wird dann übernommen und **https doa.org/artickle wird ersetz** nur identifier bleibt stehen über noch ein punkt escape dass alles xml entspricht. Weil xml erlaubt **keine umlaute, diese müssen entfernt werden**

Gibt auch eine for- schleife- für mehrer autoren




**reverse engineering**
erste feld für 001, ist schon geschireben
feld issn, die umlaute wegnehmen aber
autoren
 
Split 0 funktion was die mit Autor macht,
erste wert wird nur genommen, es wird also flavia genommen und das wird in dieses feld 100a geschreiben und dnan wieder ins xml kopiert
 
Autor 700 also diese slice schneidet den wert weg, diese wird dann in wert ins subfield code a genommen
 
	 
**Übungen Template ergänzen**
wir wollen Marc Felder füllen, Bis jetzt haben wir autor, titel, url, issn genommen

         Jetzt noch die doi nehmen, Befehle waren so:
{{
forNonBlank(
    cells['DOI'].value,
    v,
    '<datafield tag="024" ind1="7" ind2=" ">
        <subfield code="a">' + v.escape('xml') + '</subfield>
        <subfield code="2">doi</subfield>
    </datafield>',
    ''
)
}}

 
	Zuerst muss mal doi spalte anschauen, 73  wo es keine doi hat
Muss in geschweiffte klammern um abzugrenzen
 
	Gibt nur null, dann müsste jetzt eine if abfrage, wenn das feld leer, dann mach das
Oder man kann nonblade
  wens blank ist, nichts reinschreiben
 
 
	Wenns blank ist, dann nicht, und sonst datafield
 
	 
	Wenn datensätze leer sind, dann nichts und sonst doi
 

 
	Wenn das feld nicht leer ist, dann doi schreiben in feld 024? Unterfeld a2

das waren die Befehle:






 ***nun ging es wieder zurück zur angefangen Template** zum Glück standen die Befehle noch richtig drin,die ich vorher copy pastet habe. dann musste man auf ausführen (execute drücken) und es hat einem ein txt. datei gemacht. 
 
  ![Screenshot from 2021-12-03 11-43-46](https://user-images.githubusercontent.com/90834735/144604957-a4ea8c47-8069-4daa-b64b-b048191d205d.png)
 
 
 
 diese war dann verfügbar, zuerst konnte ich sie nicht speichern. habe dann aber noch hilfe bekommen und diese in den downloads gespeichert, nun galt es diese datei in xml umzubennen und danach zu speichern. man hätte separaten ordner machen sollen, weil bei der späteren Validierung hat es dann einfach alle Dokumente genommen, und diese validiert.
	ein dokument war ja von archive space in Ead- Format, das gibt’s im marc21 nicht dann gibt es fehlermeldung in der SHELL Validierung.
	
**AHA-Moment** So haben wir schon fast gültige datensätze, diese müsste man noch validieren mit xmilini und mit den Befehlen, die wir copy/pasten konnten. 
es hat also den Standard das Schema Marc21 aus dem loc.gov mit der doaj-articel-sample verglichen.

cd ~/Downloads
wget https://www.loc.gov/standards/marcxml/schema/MARC21slim.xsd
xmllint doaj-article-sample-csv.xml --noout --schema MARC21slim.xsd

so kann man prüfen ob es dem schema marc21 entspricht, hatten wir im gset sonst immer mit einem Validierungs-tool im Browswer gemacht, das war neu für mich.
 und dann erfolgt die Validierung.
![Screenshot from 2021-12-03 13-53-45](https://user-images.githubusercontent.com/90834735/144605936-041a11bf-165c-4a88-beae-dcae28f4d20f.png)


 

 
	Validierung ist wichtig auch in Nationalbibliotheken, prüft ob Reihenfolge und Zahlen und keine Buchstaben und ob das alles passt. Weil Daten manuell, gibt viele Fehler.

Validierung in der SHELL mit diesen Befehlen:
 
 
 **AHA-Moment!**

**Validierung in der SHELL /TERMINAL**
![Screenshot from 2021-12-03 12-19-13](https://user-images.githubusercontent.com/90834735/144594143-1f1a6cb5-4490-4a95-965a-b50adf2d943d.png)


 
	Dann habe es validiert, da hat es dann aber alle dokumente genommen,  aber man kann sehen, dass der doaj-article-sample-csv.xml validates geklappt hat das validieren, bei den anderen downloads die auch noch drin waren, hat die validierug nicht geklappt.
	Aber das war ja auch nicht das Ziel. Ziel war es ja das erstellte Template, dass nun xml konform zu sein scheint, auf seine Gültigkeit zu prüfen, ich wusste gar nicht, dass man das im Terminal auch kann mit einem Befehl. In der Schule im GSET Unterricht haben wir jeweils mit einem Tool im Browser (https://onlinexmltools.com/)  validiert.

Open refine speichert alles, auch wenn beednet habe


 im Ordner downloads hat es noch andere Dokumente, diese wurden halt auch mit validiert
 
