---
title: "Tag 9"
date: 2022-01-13
---
_Liebes Tagebuch_,


**Suchindexe, Praxisprojekt DLA Marbach, Unterschied BibFrame u FRBR, üben Open Refine + wikidata/query builder**
   
Die horizontale Suchmaschine erfordert kein Datenschema, erlaubt semantisch Abfragen oder Feldsuche z. b. Google, Volltextsuche. Die Vertikale, das ist ein Datenmodell dahinter, gesucht wir über homogene Datenbestände, es braucht also ein Schema
 und einen Prozess. z.b. Bibliothekskatalog, Online-Shop. 
Es gibt aber auch Suchmaschinen, die für horizontale + vertikale Suche der **Apache Solr** oder **Elasticsearch** ist stärker bei horizontale Suchszenarien.

**Primo** von Exlibris, da gibt’s auch einen zentralen Index, wo die Ressourcen in eigenes Discovery erscheinen.
Diese werden dann zugeschaltet.

**Nachteil der open Source Produkte** ist es eben, dass kein Index mitgeliefert wird, muss selber schauen, wie man das löst z.b. über Katalogisatsuche
Open source haben eben keinen eigenen index, der einen solchen Index pflegen würde, ist halt nicht kommerziell, dass muss man sich bewusst sein, wenn man open source anwendet.
 
**Marktführer, kommerzielle Suchindexe**
Die Nischenanbieter tauchen hier auf der Liste nicht auf, nur grosse Anbieter wie
- VuFind
- Primo von Ex- Libris ist dabei
- World Cat von OCLC sei aufgeblasen, weil diese Nutzer drin haben, die WorldCat nicht installiert haben, sondern nur im Browser nutzen.
- EBSCO Discovery Service
- Summen (Primo ist die gleiche Firma). Das führt dazu, dass Summen – Lösung nicht mehr verkauft wird, man soll eben dann Primo nehmen.

Ein Zentralindex für VuFind, heisst Artikelindex von Finc (Verein).
Jetzt eben ein Artikelindex für open source programme.
Seit einem Jahr online, konsolidiert.Swisscollections hat auch VuFind für Suche drin mit einem Solar Index im Hintergrund.
 
Sehr spannend fand ich das Praxisprojekt der beiden Dozenten, die den Katalog des Deutschen Literaturarchives in Marbach aufgebaut haben.
**Aus der Praxis: Deutsche Literaturarchiv in Marbach**

[Katalog DLA Marbach](https://www.dla-marbach.de/)

Es gibt ein Bibliothek, ein Archiv als auch ein Museum.
Es sammelt und erhält alles, dass was in irgendeiner Beziehung zu deutschsprachigen Literaten steht.
Es gibt auch ein Archiv mit Manuskripte  aus Nachlässen.= archivalische Architekte
Musealen Aspekt, Porträt oder prägende Gegenstände aus dem Leben der Literaten.
Gibt auch Schiller Mueum in Marbach.![Ring](https://user-images.githubusercontent.com/90834735/151678861-68d3275d-1b71-44bc-b10e-1af63f3e3bb0.png)

Museale Architekte in Datenformaten und Datenstrukturen und das gleiche gilt auch für Archive und Bibliotheke mit eigener Software, wo Erschliessung stattfindet.
Die Software kommt für alle 3 Bereiche (Archiv, Bibliothek, Museum) zum Einsatz. Software heisst [Adis](https://de.wikipedia.org/wiki/ADIS/BMS). 
Möchte ich Bilder und Objekte im Museum suchen oder Manuskript zu einem Werk dann im Gesamtkatalog suchen. Auch wenn ein Werk in Bibliothek und Manuskript ins Archiv iste, es gehört zusammen, auch wenn in 3 verschiedene Sparten verteilt ist.
Dann kann man nach Beständen, nach Namen und Personen und Körperschaften suchen.

**Für Nutzer war das ein Problem in welcher Suchmaske soll er jetzt das Objekt suchen?**
Darum brauchte man ein **Discovery wo man alles suchen kann wo man im Haus hat**, aber man soll auch explizit nach Namen und Manuskripten suchen sollen. Das war die Aufgabe für die beiden Dozenten Hr.Lohmeier und Hr. Meyer.
Sie schauten bei ***Schlagworten**.Handschriftliches Manuskript und Werk gehört dazu. Da schauten sie die Bestände an.**Open Refine** haben sie dafür genutzt um die Struktur der Daten zu sehen.
Über jede Spalte eine Facette zu bilden.
Sie haben alle Daten in einen **VuFind (Solarindex) gemacht in einem Prototyp**.
Es konnten alle Daten in ein gemeinsames System so integrieren.
Dann wurde ein Hauptprojekt gemacht.
Ein Personenfirma (Apache Solar Experte) dass er **Solarindex** macht.
Webdienstleister macht das **Webinterface** und war auch **Experte für Vufind**.
Es wurde aber **nicht mehr VuFind eingesetzt, sondern Typo 3- Find**.
Wegen Managment weil Homepage war scho in Typo 3. Das war Stand der Dinge.
Es gab schon Kompetenzen und man hatte schon Webdienstleister an der Hand für den Support und die Updates. Darum war es naheliegend diesen Katalog schon in Typo 3 zu integrieren.Sonst hätte man wieder neuen Dienstleister nehmen sollen.
**Typo 3- find ist ein open source wie vufind**
VuFind keinen zentralen Index, wo bekommt man den hin?
Weil ist keine kommerzielle Anbieter.
**Lösung wäre es hier K10-plus Zentral  (Deutschland)** das hat man für den Marbacher Katalog genommen.
Erhält nicht nur sämtliche Bestandesdaten von deutschen Bibliotheken, sondern auch noch Kollektionen von Elsevier.
Solar Index gibt es dann, wie wir es im VuFind auch schon genutzt haben.
**Solar kann dann mehr als nur einen Index haben, kann K10-plus haben und einen eigenen Index**. Der Nutzer kann so beide Indexe nutzen.
Es wurden konsistent **Normdaten aus der GND **genommen!
Alle Personen sind mit einer **Normdaten- ID** verknüpft.
Wenn man exakt weiss, dass der Goethe der gleiche Goethe ist wie aus anderen Datensatz, auch wenn der eine im Archiv und der andere in Bibliothek zu finden ist.

**Suche im Katalog**
Man bekommt Suchvorschläge (Walter von Vogelweide wurde mal gesucht)
Dass sind Begriffe, die im Index stehen und mit Goethe beginnen.
Johann Wolfgang von Goethe ist relevanteste Treffer
Oder Werknormdaten ID gibt es auch
Goethe als Person suchen, im Hintergund wird nach ID gesucht man sucht nach ID und nicht Begriff Johann Wolfang von Goethe. 
Hasenclever ist mit Goethe verknüpft, weil es eine Widmung an Goethe war.
 
**Besonderheit: Normdatensätze angereichert**
Goethe Faust kommt dann, das berühmte Faust
Eule gesucht, kein Normdatum, 532 Treffer
Mal ist Eule im Titel, im Abstract oder  so drin aber ist ein Treffer.
**Jetzt welche Normdaten sind damit verknüpft?**
Wenn Eule im Verlag, dann taucht der Verlag auf bietet Einstieg alles zu dieser Person
Hat Pseudonyme drin, weil unter anderem Namen publiziert.
Alle diese Info stammen aus GND oder Marbacher Forschungsarbeiten.
Wikipedia und Wikidata Datensätze haben sie auch genommen und eingespielt.
Normdatensätze können auch als Einstieg genommen werden für die Suche.
 
**Gemeinsamer Suchindex**:Unter Wegner= Nachlassbestände, das was er hinterlassen hat
Alles zu dieser Person dass wäre dann vergleichbar wie mit dem Suchschlitz.
Marbach hat einen gemeinsamen Index also Archiv, Bibliothek und Museum in einem gemeinsamen Index überführt.
Bestände sind nicht abgeschlossen, sie erschliessen jeden Tag.

**Datenprozessierung: Daten homogenisieren, vertikalen Suchmaschinen damit in Suchschema passen.**
Muss Daten vorbereiten, dass in Suchindex überführt werden.
Dass muss täglich einmal durchlaufen werden für Gesamtbestand.
**Das sind 3,6 Mio Daten täglich!! Das geht über Nacht**
Unstrukturierte Textdateien mit Software namens Panda, werden in ein Tabellenformat aufbereitet, so dass  dann mit openrefine noch aus daten von wikipedia etc.
Alles wird aufbereitet z. b. Datumsbereiche für zeitliche Facette
Dann wird in TSV Format ähnlich CSV wieder in Solar Index eingespielt.
Einmal täglich!
Für Datenprozessierung muss so gemacht werden, dass Daten in zweite Index eingespielt werden, erst wenn alter Index gemacht wurde, wird umgeschaltet auf neuen Index. 
=Sicherheit, dass ein Katalog sicher funktioniert.

**AHA-Moment: Alles war wir im Modul kennengelernt haben, haben sie in der Praxis auch angewendet am Beispiel von Marbach Katalog.**

Es wurde noch ein Beispiel auf Open Refine gemacht.
**AHA-Moment**: Wikidata hätte man auch mit open refine gearbeitet. Stimmt ich sehe die Wikidata bei der Übung mit den 11 Autoren!

![reconcilation](https://user-images.githubusercontent.com/90834735/151678011-3b47a061-5f06-4c90-904c-a68219450890.png)


Man sollte Human (Mensch) anwählen als Property:
![human](https://user-images.githubusercontent.com/90834735/151678014-43ec78f7-24ed-4750-bf3c-d6e1b5dbabf6.png)


dann kamen die Properties Mensch dazu: ![mensch](https://user-images.githubusercontent.com/90834735/151678024-9cd6c5f4-60b7-4eec-892f-e642a82d5a34.png)



James Baldwin Zelle 2 soll man nehmen gemäss der Aufgabe
![match](https://user-images.githubusercontent.com/90834735/151678019-3507a4b6-04d9-47fd-8434-d7026ac1735c.png)


In Cell 10, Click on the first name, then the second name. Do you see an African-American writer? Choose him by clicking the corresponding single check-mark
dann musste man diese zwei Zellen miteinander matchen.
![match2](https://user-images.githubusercontent.com/90834735/151678080-4e0a145b-476c-4c00-8ca2-2ae58da03621.png)

dann kam das heraus:

![match wright](https://user-images.githubusercontent.com/90834735/151678072-17edba1e-adf5-452f-b2aa-bf29b67ad26c.png)


die zweite Aufgabe war es Geburts-und Todesdatum zu den Autoren zu ergänzen, das ging auch wieder über die Reconilation aber über die Values:
![geb datum](https://user-images.githubusercontent.com/90834735/151678089-8edc9016-2b9c-48db-84ff-32c916ef4cd5.png)


dann habe ich es überprüft und es erschienen zwei Daten:
![birth_death](https://user-images.githubusercontent.com/90834735/151678100-81dde3ef-ed89-4b4a-b756-e295a911e044.png)

![jetzt mit birth u  death](https://user-images.githubusercontent.com/90834735/151678102-81e29868-7152-44da-b2a5-0a7229f9ff00.png)



**Wikidata: mit Suchanfrage ähnlich wie Sparql**
 
96 Mio Datensätze auch elektronische Artikel  haben mit Mensch u. Erde angefangen.
Wikipedia ist ja das grösste Projekt von ihnen.Es soll aber nur einmal die Daten abglegt werden, in einer grossen strukturierten Datenbank.
Mensch war Q1
Erde =Q2 
Dann eben Q1776290…hochgezählt. Ein Besipiel mit Carina Dahl wurde gemacht.
 
 
Wikidata Anfrage= Query
Ein spannende Zusammenfassung über die Anfrage habe ich im Wikipedia gefunden:
![wikidata query service](https://user-images.githubusercontent.com/90834735/150576371-34d44f5b-7340-496b-b449-9fc58ff424e0.png)


  
![Screenshot from 2022-01-13 19-35-09](https://user-images.githubusercontent.com/90834735/149389924-e2d1b2a8-30b8-4223-85d5-31f8b69cdac2.png)

Der Query Builder scheint wohl für alle gedacht zu sein, die nicht gern Sparql-Anfragen schreiben, so wie es auf mich zutrifft. 

In Zukunft wird man noch mehr auf Wikidata setzen in Bibliothek und Archiv. In Marbach hat man ja Links auf Bilder dann genommen, man sollte aber Wikdata prüfen.
Aber Personenlebensdaten sollte man noch einen Plausabilitätscheck nehmen.


Tschau liebes Tagebuch!
