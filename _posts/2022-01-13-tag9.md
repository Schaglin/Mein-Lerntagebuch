---
title: "Tag 9"
date: 2022-01-13
---
_Liebes Tagebuch_,
   
 Zuerst bekamen wir mal eine Erklärung zu **Suchmaschinen**. Die horizontale Suchmaschine erfordert kein Datenschema, erlaubt semantisch Abfragen oder Feldsuche z. b. Google, Volltextsuche. Die Vertikale, das ist ein Datenmodell dahinter, gesucht wir über homogene Datenbestände, es braucht also ein Schema
 und einen Prozess. z.b. Bibliothekskatalog, Online-Shop. 
Es gibt aber auch Suchmaschinen, die für horizontale + vertikale Suchszenarien eingesetzt werden hier kann **Apache Solr (Suchmaschine)**
genannt werden. Sie ist aber stärker bei der vertikalen Suche, wo es ein Datenmodell dahinter hat.Hingegen die **Elasticsearch (Suchmaschine)** ist stark bei horizontale Suchszenarien.

**Primo** von Exlibris, da gibt’s auch einen zentralen Index, wo die Ressourcen in eigenes Discovery erscheinen.
Diese werden dann zugeschaltet aus diesem Zentralindex.

**Nachteil der open Source Produkte** ist es eben, dass kein Index mitgeliefert wird, muss selber schauen, wie man das löst z.b. über Katalogisatsuche
Open source haben eben keinen eigenen index, der einen solchen Index pflegen würde, ist halt nicht kommerziell, dass muss man sich bewusst sein, wenn man open source anwendet.
 
**Die Platzhirsche (Marktführer,kommerzielle Suchindexe)**
Die Nischenanbieter tauchen hier auf der Liste nicht auf, nur grosse Anbieter wie
- VuFind die Finna (finnische Nationalbibliothek) basiert auf VuFind
- Primo von Ex- Libris ist dabei
- World Cat von OCLC sei aufgeblasen, weil diese Nutzer drin haben, die WorldCat nicht installiert haben, sondern nur im Browser nutzen.
- EBSCO Discovery Service
- Summen (Primo ist die gleiche Firma)
 
**Primo hat Summen verdrängt**
Summen zählt eigentlich zu Primo auch zu Ex Libris.
Pro Quest ist ursprüngliche Anbieter von Summen.
Das führt dazu, dass Summen – Lösung nicht mehr verkauft wird, man soll eben dann Primo nehmen.
 

 
**VuFind in Ex-Libris, Swisscollections steckt die Suchmaschine drin**
Ein Zentralindex für VuFind, heisst Artikelindex von Finc. Finc ist ein Verein von der Rechtsnorm.
Jetzt eben ein Artikelindex für open source programme, dann kann man eben open source nehmen, weil man doch einen Index hat, zwar keinen kommerziellen Zentralindex.
 
**Zentralindex von Ex-Libris**
Seit einem Jahr online, konsolidiert.
Swisscollections gibt’s auch für Sammlungen, durchsuchbar machen. Digitalisierte Bestände herausgreifen und durchsuchbar machen.
alle Einrichtungen, die was zu Goethe haben. ZB Zürich, Basel…
Swisscollections vielleicht besser bedient als mit Swisscovery für digitale.
Swisscollections hat auch VuFind für Suche drin! Was noch erweitert wurde von der Unibibliothek. Demnach auch mit einem Solar Index im Hintergrund.
 


**Deutsche Literaturarchiv in Marbach**
Es gibt ein Bibliothek, ein Archiv als auch ein Museum.
Es sammelt und erhält alles, dass was in irgendeiner Beziehung zu deutschsprachigen Literaten steht.
Es gibt auch ein Archiv mit Manuskripte  aus Nachlässe.= archivalische Architekte
Musealen Aspekt, Porträt oder prägende Gegenstände aus dem Leben der Literaten.
Gibt auch Schiller Mueum in Marbach.
Museale Architekte in Datenformaten und Datenstrukturen und das gleiche gilt auch für Archive und Bibliotheke mit eigener Software, wo Erschliessung stattfindet.
Die Software kommt für alle 3 Bereiche im gleichen Software von der Adis.
-Bibliotheksmodul
-Archivmodul
-Museumsmodul
Auch im Adis. Möchte ich Bilder und Objekte im Museum suchen oder Manuskript zu einem Werk dann im Gesamt suchen auch wenn Werk in Bibliothek und Manuskript im archiv …gehört zusammen, auch wenn in 3 verschiedene Sparten verteilt ist.
Dann kann man nach Beständen suchen.
Oder man kann nach Namen und Personen und Körperschaften suchen.
Für Nutzer war das ein Problem in welcher Suchmaske soll ich suchen?
Darum brauchte man ein Discovery wo man alles suchen kann wo man  im Haus hat, aber man soll auch explizit nach Namen und Manuskripten suchen sollen. Das war die Aufgabe für Hr.Lohmeier und Hr. Meyer.
Sie schauten bei Schlagworten.Handschriftliches Manuskript und Werk gehört dazu. Da schauten sie  Bestände an.Open Refind haben sie dafür genutzt um die Struktur der Daten zu sehen.
Über jede Spalte eine Facette zu bilden.
Sie haben alle Daten in einen VuFind (Solarindex) gemacht in einem Prototyp.
Es konnten alle Daten in ein gemeinsames System so integrieren.
Dann wurde ein Hauptprojekt gemacht.
Ein Personenfirma (Apache Solar Experte) dass er Solarindex macht.
Webdienstleister (Webinterface und auch Experte für Vufind)
Es wurde aber nicht mehr VuFind eingesetzt, sondern Typo 3- Find.
Wegen Managment weil Homepage war scho in Typo 3. Das war Stand der Dinge.
Es gab schon Kompetenzen und man hatte schon Webdienstleister an der Hand SUppoert und Updates. Darum war es naheliegend diesen Katalog schon in Typo 3 zu integrieren.
Sonst hätte man wieder neuen Dienstleister nehmen sollen.
**Typo 3- find ist ein open source wie vufind**
VuFind keinen zentralen Index, wo bekommt man den hin?
Weil ist keine kommerzielle Anbieter.
**Lösung wäre es hier K10-plus Zentral  (Deutschland)** das hat man für den Marbacher Katalog genommen.
Erhält nicht nur sämtliche Bestandesdaten von deutschen Bibliotheken, sondern auch noch Kollektionen von Elsevier
Solar Index gibt es dann, wie wir es im VuFind auch schon genutzt haben.
Solar kann dann mehr als nur einen Index haben, kann K10-plus haben und einen eigenen Index. Der Nutzer kann so beide Indexe nutzen.
 
 
Es wurden konsistent Normdaten aus der GND genommen!
Alle Personen sind mit einer Normdaten- ID verknüpft.
Wenn man exakt weiss, dass der Goethe der gleiche Goethe ist wie aus anderen Datensatz, auch wenn der eine im Archiv und der andere in Bibliothek zu finden ist.
 
Man bekommt Suchvorschläge (Walter von Vogelweide wurde mal gesucht)
Dass sind Begriffe, die im Index stehen und mit Goethe beginnen.
Johann Wolfgang von Goethe ist relevanteste Treffer
Oder Werknormdaten ID gibt es auch
Goethe als Person suchen , im Hintergund wird nach ID gesucht man sucht nach ID und nicht Begriff Johann Wolfang von Goethe. 
 
Hasenclever ist mit Goethe verknüpft, weil es eine Widmung an Goethe war
 
**Besonderheit: Normdatensätze angereichert**
 Goethe Faust kommt dann, das berühmte Faust
Eule gesucht, kein Normdatum 532 Treffer
Mal ist Eule im Titel, im Abstract oder  so drin aber ist ein Treffer.
Jetzt welche Normdaten sind damit verknüpft?
Wenn Eule im Verlag, dann taucht der Verlag auf bietet Einstieg alles zu dieser Person
Hat Pseudonyme drin, weil unter anderem Namen publiziert.
Alle diese Info stammen aus GND aber vieles auch in Marbach in Rahmen von Forschungsarbeiten ermittelt worden.
Wikipedia Datensätze haben sie auch genommen und eingespielt.
Normdatensätze können auch als Einstieg genommen werden für Suche.
 
Unter Wegner= Nachlassbestände, das was er hinterlassen hat
Alles zu dieser Person dass wäre dann vergleichbar wie mit dem Suchschlitz.
 
Alles was er an Bilder dem Archiv vermacht hat suchen z. B. 17  Kasten Material, auch eine Totemmaske gehörte zum Bestand dazu.

Marbach hat einen gemeinsamen Index also Archiv, Bibliothek und Museum in einem gemeinsamen Index überführt.
Bestände sind nicht abgeschlossen, sie erschliessen jeden Tag.
Datenprozessierung: Daten homogenisieren, vertikalen Suchmaschinen damit in Suchschema passen.
Muss Daten vorbereiten, dass in Suchindex überführt werden.
Dass muss täglich einmal durchlaufen werden für Gesamtbestand.
Das sind 3,6 Mio Daten täglich!! Das geht über Nacht
Unstrukturierte Textdateien mit Software namens Panda, werden in ein Tabellenformat aufbereitet, so dass  dann mit openrefine noch aus daten von wikipedia etc.
Alles wird aufbereitet z. b. Datumsbereiche für zeitliche Facette
Dann wird in TSV Format ähnlich CSV wieder in Solar Index eingespielt.
Einmal täglich!
Für Datenprozessierung muss so gemacht werden, dass eben Daten in zweite Index.
Eingespielt werden, erst wenn alter Index gemacht wurde, wird umgeschaltet auf neuen Index. 
Sicherheit, dass ein Katalog sicher funktioniert.

**AHA-Moment: Alles war wir im Modul kennengelernt haben, haben sie in der Praxis auch angewendet am Beispiel von Marbach Katalog.**




 
Frage: **Sind in dem Katalog auch elektronische Daten verzeichnet?
Sonstiger Datenträger
Da gibt es CD Rom und Usb Stick aus versch. Leute
 
Über Bestände gehen.
 
Also aus modernen Nachlässen?
Ja unter Beispiel Raddatz Fritz Hier gibt’s CD’s auch,	Fotokonvolut**

Datentankstelle ist geplant.
 
 
**Anderes Projekt Schaufenster_projekt für open access***
 	
Einzelseite wird angezeigt hier kennt man aus Retrodigitalisate
Born digitales pdf sind in einer Volltextsuche verzeichnet.
Auch als pdf wird Text herausgelesen!
 

	 
**Datentransformation: Über Schnittstellen wurden aus OAI-PMH Schnittstelle geladen und in Format Dublin Core herunterladen dann wurde in METS MOTS transformiert**
 
Diese Datenquelle wurde auf GitHub abgelegt.
Sind auf dem Server so 20 Zeitschriften mit aktuellen Ausgaben.
Ein ganz normaler Zeitschriftenserver da gibt es eine OAI-PMH Schnittstelle
Aufgabe von Lohmeier war es dann für jede Zeitschrift eine OAI
Dann eben Dublin Core wurde ein Mapping auf METS MOTS gemacht
Es gibt einen Code auf git hub dazu und auch die Regeln zum konvertieren finden sich auf GitHub.
Diese werden dann auch wieder über eine OAI-PMH an Partnerfirma übermittelt.
Git Hub Actions gehört zu Microsoft
Ganz ohne eigene Server zu betreiben, lassen sie jetzt diese Prozesse täglich ablaufen
Haben git scrapping benutzt, also laden Daten hinunter und speichern quch Quelldaten Datensatz aus der Zeitschrift wird dann mit open refine konvertiert
Nachts wird gemacht, 500 stunden über git hub action sind gratis.
 
Git Scrapping: Feuer in Kalifornien Daten kann man auch über git scrapping ganze Historien anzeigen lassen.

Er sieht jetzt was geändert wurde hier wurde ein Abstract korrigiert.
Inhaltliche Änderungen kann man nachvollziehen.
 


**Datenreicherung mit open refine**
Habe ich gemacht die übung
-Place of birth, date
-11 Autoren suchen da…
-Mark Baldwin amerikanischer Schriftsteller..
-Reconillation machen.
 
**Vorstellung Bib Frame**
Bib Frame Modell (Nachfolger von Marc 21. Also dem Ableger Mods schlanke Form von Marc 21)
Für bibliothekarischen Datenaustasuch
War von Library of Congress, Washington
FRBR   und RDA basierend gemeinsames Regelwerk für biblio und Archiv
Werden aber von Bib Frame nicht zu 100% umgesetzt, gibt immer noch Sachen die nicht gehen
Wird Bib Frame erweitert
Das Datenmodell ist eben auf FRBR basiert ,unterscheidet von Werken,Physisches Objekt. 3 Manifestationen.




**Wie ist die Beziehung von BIBFRAME zu FRBR?**
Dort sind ja vier Ebenen definiert. Bei BIBFRAME fehlt ja die Expression
 Wikipedia sieht auch diesen Design-Unterschied: https://de.wikipedia.org/wiki/BIBFRAME#Design

BIBFRAME ist in RDF verfasst und basiert auf drei zentralen Abstraktionsebenen (Work, Instance, Item), mit drei zusätzlichen Klassen (Agent, Subject, Event) die mit den zentralen Kategorien verbunden sind. Während sich Work in BIBFRAME als Zusammenführung der Kategorien Work und Expression im FRBR-Modell der IFLA begreifen lässt, entspricht die Kategorie Instance in BIBFRAME der Kategorie Manifestation in FRBR. Hierbei handelt es sich um einen Bruch mit FRBR und den FRBR-basierten Resource Description and Access (RDA) Katalogisierungsregeln.
 
In Marbach habe sie noch kein BibFrame. 
Standards von Bib Frame und klassischem Format.
Wenn Felder dann umbenennen, die Tabellen, wie bei Bib Frame heissen, dann ist es leichter ein Mapping auf Bib Frame Format zu machen.
 

Bei BibFrame: Titel und Subclassen erben die Eigenschaften der Oberklasse (Mutter). Bei BibFrame geht es um Beziehungen.
Ontologie umfasst Beschreibungsklassen (Class), die spezifischen Eigenschaften(Property )verfügen.

Wenn nur ein Marc21 vor mir haben, solle ich alle Infos haben, alle sind aggregiert über Werk und Manifestation und all das zusammen in einem Datensatz. Bezeichnung für Personennamen, ohne weitere Quellen  kann so den Datensatz lesen. Der GND Identifier ist nicht drin.aber Marc sind halt Zahlen 205 A etc. wenn man nicht weiss, wass die Feldbezeichung heisst, dass ist in Bib Frame besser gelöst.
Bib Frame jede Entität wird alleine genommen. Man braucht vielzahl für die Gesamtbeschreibung.
Sehr  komplexe Beziehungskonflekt, aber man hat auch die Beziehungen drin, also so ein Autoren name findet man in Marc21 1000 Fach und bei Bib frame gibt’s genau einmal die Entität die diesen Autor beschreiben und diese ist dann einfach 1000 x verknüpft mit anderen Objekten (Werken, Manifestaitonen etc. dazu).

Wechsel auf Bib Frame noch nicht vollzogen, aber ist Zukunft.
Wie beim Telefon, je weiter umso schneller wird sich durchsetzten, dass zu adaptieren.

**Anmerkung zu Alma** 
Alles was auf Marc 21 basiert, da kann man Person einmal anlegen und dann mit Datensätzen verknüpft. Gibt ja Oberfläche aber im Hintergrund, speichert zu jedem Datensatz die Person.
(Bei BibFrame wäre dass dann anderst, weil ja Beziehung und nur einmal ein Datensatz.)
Man muss also in Marbach beispielsweise der Autor dann 10'000 mal der Datensatz geändert werden.
Verbundzentrale in Deutschland, durch mehrfache Datenkonfigurationen, diese Marc21 Datensätze mit gleicher GND-Identifier dass dann nicht mehr 100% steht und bei BIB Frame wird es erzwungen durch das Format, darum bessere Qualitätskontrolle.
Man muss also Funktionen und Format, die Datensätze… aber auch ein auf MARc21 basiertes Programm, kann gut sein.

**Ric (Records in Context)**
Das Bib Frame des Archivwesens
Basiert auch auf Linked Data Prinzipien.
Gibt eine schweizerische Projektgruppe, arbeitet eine Projektgruppe (Niklaus Stettler) mitbeteiligt.
Wir haben auch hier Events, Functions, Agents, Place, aber bei Bib Frame Bibliotheken ist einfacher strukturiet. Bei Bib Frame ist Werk im Hintergrund bei Ric ist Entstehungskonzept wichtig. Wie ist es entstanden.
Entwicklungsstand
0.2 veröffentlicht
0 ist noch nicht stabil
Bibframe ist ja schon bei 2.0

 
 
**Atom**
haben wir einmal in einer Übung in einem früheren Archivmodul kennenglernt. Atom ist ähnlich vielleicht dann wird wie alma für Ric
Atom ist schon gemacht
Gemäss Niklaus Stettler, dass haben wir ja mal gesehen.
Man wird grosse Projekte haben.
Erst in 5 Jahren meint Hr. Stettler wird sich Ric etablieren.
Erschliessungspraxis wird sich ändern stärker, weil Trend zu Normdaten (GND) da gibt’s auch dass die Archive diese dann verwenden.
Hochschularchiv der ETH nimmt auch GND anscheinend.






**AHA-Moment**: Wikidata hätte man auch mit open refine gearbeitet. Stimmt ich sehe die Wikidata bei der Übung mit den 11 Autoren!


**übung open refine**



**Prinzip von open refine**
![open refine prinzip](https://user-images.githubusercontent.com/90834735/150575719-299a0d89-abda-4bd7-9593-6a1bf53fa07d.png)




auf dem Terminal wurde wieder das open refine geöffnet, es musst nicht mehr heruntergeladen werdenn, es war ja noch installiert von tag 5.

Hier die Befehle:
kuengjacquel@PCVDBAIN207:~$ wget https://github.com/OpenRefine/OpenRefine/releases/download/3.5.0/openrefine-linux-3.5.0.tar.gz
2022-01-13 10:14:08 (9.23 MB/s) - ‘openrefine-linux-3.5.0.tar.gz.1’ saved [131840764/131840764]

kuengjacquel@PCVDBAIN207:~$ cd ~/openrefine-3.5.0
kuengjacquel@PCVDBAIN207:~/openrefine-3.5.0$ ./refine


dann wurden die Daten (11 Autoren) (in Format csv) über die URL geladen.
Create Project > Web Addresses (URLs)
        https://raw.githubusercontent.com/libjohn/openrefine/master/data/AA-authors-you-should-read.csv
        Next
 das Projekt kann mit eine Namen abgespeichert werden.
    Create Project
  Die Ansicht wurde noch geändert,auf zeige 25, dass man alle 11 Autoren sehen kann.
![11 Autoren](https://user-images.githubusercontent.com/90834735/151678000-c6feb3c5-b3ea-4019-87be-8678878529a8.png)


Start Reconciliation:
![reconcilation](https://user-images.githubusercontent.com/90834735/151678011-3b47a061-5f06-4c90-904c-a68219450890.png)


Man sollte Human (Mensch) anwählen als Property:
![human](https://user-images.githubusercontent.com/90834735/151678014-43ec78f7-24ed-4750-bf3c-d6e1b5dbabf6.png)


dann kamen die Properties Mensch dazu: ![mensch](https://user-images.githubusercontent.com/90834735/151678024-9cd6c5f4-60b7-4eec-892f-e642a82d5a34.png)



James Baldwin Zelle 2 soll man nehmen gemäss der Aufgabe
![match](https://user-images.githubusercontent.com/90834735/151678019-3507a4b6-04d9-47fd-8434-d7026ac1735c.png)


In Cell 10, Click on the first name, then the second name. Do you see an African-American writer? Choose him by clicking the corresponding single check-mark
dann musste man diese zwei Zellen miteinander matchen.
![match2](https://user-images.githubusercontent.com/90834735/151678080-4e0a145b-476c-4c00-8ca2-2ae58da03621.png)

dann kam das heraus:

![match wright](https://user-images.githubusercontent.com/90834735/151678072-17edba1e-adf5-452f-b2aa-bf29b67ad26c.png)


die zweite Aufgabe war es Geburtstdatum und Todesdatum zu den Autoren zu ergänzen, das ging auch wieder über die Reconilation aber über die Values:
![geb datum](https://user-images.githubusercontent.com/90834735/151678089-8edc9016-2b9c-48db-84ff-32c916ef4cd5.png)


dann habe ich es überprüft und es erschienen zwei Daten:
![birth_death](https://user-images.githubusercontent.com/90834735/151678100-81dde3ef-ed89-4b4a-b756-e295a911e044.png)

![jetzt mit birth u  death](https://user-images.githubusercontent.com/90834735/151678102-81e29868-7152-44da-b2a5-0a7229f9ff00.png)





**Wikidata: mit Suchanfrage ähnlich wie Sparql**
 
96 Mio Datensätze auch elektronische Artikel  haben mit Mensch u. Erde angefangen.
Wikipedia ist ja das grösste Projekt von ihnen
Es soll aber nur einmal die Daten abglegt werden, in einer grossen strukturierten Datenbank.
Mensch war Q1
Erde =Q2 
Dann eben Q1776290…hochgezählt siehe bei Carina Dahl
 
Wurde importiert aus englischen Wikipedia, gibt aber auch japanische wikidata..ist sehr breit aufgestellt.
 
Wikidata Anfrage= Query


Ein spannende Zusammenfassung über die Anfrage habe ich im Wikipedia gefunden:
![wikidata query service](https://user-images.githubusercontent.com/90834735/150576371-34d44f5b-7340-496b-b449-9fc58ff424e0.png)


  
**AHA-Moment: **Viele Identifier**
-Viaf Identifiert 
-ISNI Identifier
-All Music Artist ID Identifier 
-Facebook ID Identifier
-YouTube Channel Identifier!

Beispiel mit Carina Dahl wurde gezeigt:
Carina Dahl ist a Instance of Mensch.
Oder Carina Dahl is property Female…

**Wikidatas-Query Builder**
![Screenshot from 2022-01-13 19-35-09](https://user-images.githubusercontent.com/90834735/149389924-e2d1b2a8-30b8-4223-85d5-31f8b69cdac2.png)

Der Query Builder scheint wohl für alle gedacht zu sein, die nicht gern Sparql-Anfragen schreiben, so wie es auf mich zutrifft. Die Aufgabe war es Carina Dahl zu suchen.
Der Builder erscheint mir einfacher als eine Anfrage wie bei Sparql händisch zu schreiben. Ich mache häufig Flüchtigkeitsfehler, daher liegt mir Sparql nicht so,da war mir der Builder sympathischer.

In Zukunft wird man noch mehr auf Wikidata setzen in Bibliothek und Archiv. In Marbach hat man ja Links auf Bilder dann genommen, man sollte aber Wikdata prüfen.
Aber Personenlebensdaten sollte man noch einen Plausabilitätscheck nehmen.
 

Wir haben im Unterricht noch **Schiller** als Beispiel genommen.
Interessant zu erfahren ist, dass man auch seine Einflüsse, seine Signatur (Unterschrift) sehen kann, die Identifier zum Teil sogar dem Japanischen Wikipedia stammen. DAs Foto könnte man jetzt nehmen, weil es keine Urheberrechte darauf gibt. (für Katalog Marbuch haben sie dies genommen).
Wann er geboren und gestorben (an Tuberkulose) ist, welche Werke er gemacht hat. Er wurde von Goethe und Kant beeinflusst. Wo er studiert hat, so seine Archive sind. Die zahlreichen Identifier sind zu sehen.

Wikipedia ist mit Wikidata je verknüpft. 


**AHA-Moment: **Urheberrechtsfreie Bilder gibt’s auch auf wikidata weil laufen unter Creative Common Lizenz**
Das konnte auch in Marbach dann eingefügt werden, weil kein urheberschutz drauf war.
 
 

Wird viel mit Bots gearbeitet, weil Daten automatisiert.
WIKIDATA mehr Daten nun als bei GND!
Ist zwar umstritten, weil jeder kann
Auch  wie bei wikipedia kann jeder mitmachen
Brockhause wurde eingestellt, wikipedia hat überholt. Obwohl nur brockhause nur Experten schreiben
Aber wikidata kann GND dann auch überholen.



Sparql kann mit Tool Open Refine abfragen
Sparql eine gute Datenquelle auch für Zukunft!

 

Wir bekamen noch die Info, dass ein halbes Jahr das Konto auf den virtuellen Maschinen noch laufen. Ev. könnte ich diese noch einmal gebrauchen.


     
    
    
    
    Diese Quellen sollte man sich einmal anschauen, ausser die Datenschule (diese ist nicht mehr so aktuell)
    
    Library Carpentry: https://librarycarpentry.org/lessons/
    Programming Historian: https://programminghistorian.org/en/lessons/
    openHPI: https://open.hpi.de/courses
    Datenschule: https://datenschule.de/lernmaterialien/

Tschau liebes Tagebuch!
