**Tag 9 **
   
   
   
 **Erklärung Horizontale und Vertikale Suchmaschine**: 
 horizontale: erfordert kein Datenschema, erlaubt semantisch Abfragen oder Feldsuche
 z.b. Suchen im Internet (Google?), Volltextsuche
 
 Vertikale: das ist ein Datenmodell dahinter, gesucht wir über homogene Datenbestände, es braucht also ein Schema
 und einen Prozess.
 z.b. Bilbliothekskatalog, Online-Shop 

Apache Solr (Suchmaschine)
für horizontale und vertikale Suchszenarien aschine eingesetzt werden
ist aber vertikal stärker

Elasticsearch (Suchmaschine)--> stark bei horizontale Suchszenarien

Primo gibt’s auch einen zentralen Index, wo die Ressourcen in eigenes Discovery erscheinen
Diese werden dann zugeschaltet aus diesem Zentralindex

*Für open source produkte von vufind, hat ma keinen index mitgeliefert, muss selber schauen, wie man das löst z.b. über katalogisatsuche*
 
Nischenanbieter tauchen hier nicht auf
Nur grosse Anbieter wie
VuFind die Finna (finnische Nationalbibliothek) basiert auf VuFind
Primo von Ex- Libris ist dabei
World Cat von OCLC
EBSCO Discovery Service
 


	 
	Open source haben eben keinen eigenen index, der einen solchen index pflegen würde, ist halt nicht kommerziell, dass muss man sich bewusst sein, wenn man open source anwendet.
 
	

 
	Summon, Primo ist gleiche Firma
Ebsco sind die grossen Anbieter
Hinten die Zahlen wieviele neue Verträge und Installationen es gegeben hat
Primo ist ein Platzhirsch!
Worldcat aufgeblasen, da sind auch Nutzer drin, die einfach gar nicht installiert haben nur auf Browser.
 

Summen zählt eigentlich zu Primo auch zu Ex Libris
Pro Quest ist ursprüngliche Anbieter von Summen
Das führt dazu, dass Summen – Lösung nicht mehr verkauft wird, man soll eben dann Primo nehmen.
 
	Typo 3- find ist ein open source wie vufind

VUFind keinen zentralen Index, wo bekommt man den hin?
Weil ist keine kommerzielle Anbieter.
**Lösung wäre es hier K10-plus Zentral  (Deutschland)**
Erhält nicht nur sämtliche Bestandesdaten von deutschen Bibliotheken, sondern auch noch Kollektionen von Elsevier
Solar Index gibt es dann, wie wir es im VuFind auch schon genutzt haben
Solar kann dann mehr als nur einen Index haben, kann K10-plus haben und einen eigenen Index. Der Nutzer kann so beide Indexe nutzen.




 
**VuFind in Ex-Libris, Swisscollections steckt die Suchmaschine drin**
Ein Zentralindex für VuFind, heisst Artikelindex von Finc. Finc ist ein Verein von der Rechtsnorm.
Jetzt eben ein Artikelindex für open source programme, dann kann man eben open source nehmen, weil man doch einen index hat, zwar keinen kommerziellen Zentralindex.
 
Zentralindex von Ex-Libris
Seit einem Jahr online, konsoldidiert
Swisscollections gibt’s auch für Sammlungen, durchsuchbar machen. Digitalisiter Bestände herausgreifen und durchsuchbar machen
 
  alle Einrichtungen, die was zu Goethe haben. ZB Zürich, Basel…
Swisscollections vielleicht besser bedient als mit Swisscovery für digitale.
Swisscollecitons hat auch VuFind für Suche drin.! Was noch erweitert wurde von der Unibibliothek . Demnach auch mit einem Solar Index im Hintergrund.
 


**Deutsche Literaturarchiv in Marbach **
Es gibt ein Bibliothek, ein Archiv als auch ein Museum.
Es sammelt und erhält alles, dass was in irgendeiner Beziehung zu deutschsprachigen Literaten steht.
Es gibt auch ein Archiv mit Manuskripte  aus Nachlässe.= archivalische Architekte
Musealen Aspekt, Porträt oder prägende Gegenstände aus dem Leben der Literaten.
Gibt auch Schiller Mueum in Marbach.

Museale Architekte in Datenformaten und Datenstrukturen und das gleiche gilt auch für Archive und Bibliotheke mit eigener Software, wo Erschliessung stattfindet.
Die Software kommt für alle 3 Bereiche im gleichen Software von der Adis
Bibliotheksmodul
Archivmodul
Museumsmodul
Auch im Adis (wie Pestallozzi Bibliothek?)
 
 möchte ich Bilder und objekte im Museum suchen oder Manuskript zu einem Werk dann im Gesamt suchen auch wenn Werk in bibliothek und manuskript im archiv …gehört zusammen, auch wenn in 3 verschiedene Sparten verteilt ist.
Dann kann man nach Beständen suchen
Oder man kann nach Namen und Personen und Körperschaften suchen
Für Nutzer war das ein Problem in welcher Suchmaske soll ich suchen?
Darum brauchte man ein Discovery wo man alles suchen kann wo man  im haus hat, aber man soll auch explizit nach Namen und Manuskripten suchen sollen. Das war die Aufgabe für Lohmeier und herr meyer.
Sie schauten bei Schlagworten.Handschriftliches Manuskript und Werk gehört dazu. Da schauten sie  Bestände an.Open Refind haben sie dafür genutzt um die Struktur de rDaten zu sehen.
Über jede Spalte eine Facette zu bilden.
Sie haben alle Daten in einen VuFind (Solarindex) gemacht in einem Prototyp.
Es konnten alle Daten in ein gemeinsames System so integrieren.
Dann wurde ein Hauptprojekt gemacht.
Ein.Personenfirma (Apache Solar Experte) dass er Solarindex macht.
Webdienstleister (Webinterface und auch Experte für Vufind)
Es wurde aber nicht mehr VuFind eingesetzt, sondern Typo 3- Find.
Wegen Managment weil Homepage war scho in Typo 3. Das war Stand der Dinge.
Es gab schon Kompetenzen und man hatte schon Webdienstleister an der Hand SUppoert und Updates. Darum war es naheliegend diesen Katalog schon in Typo 3 zu integrieren.
Sonst hätte man wieder neuen Dienstleister nehmen sollen.

 

 jetzt so


 
	 
	Es wurden konsistent Normdaten aus GND genommen!
Alle Personen sind mit einer Normdaten- ID verknüpft.
Wenn man exakt weiss, dass der Goethe der gleiche Goethe ist wie aus anderen Datensatz, auch wenn der eine im Archiv und der andere in Bibliothek zu finden ist.
 
Man bekommt Suchvorschläge (Walter von Vogelweide wurde mal gesucht)
Dass sind Begriffe, die im Index stehen und mit Goethe beginnen.
Johann Wolfgang von Goethe ist relevanteste Treffer
Oder Werknormdaten ID gibt es auch
Goethe als Person suchen , im IHintergund wird nach ID gesucht man sucht nach ID und nicht Begriff Johann Wolfang von Goethe. 
 
	Hasenclever ist mit Goethe verknüpft, weil es eine Widmung an Goethe war
 
Besonderheit
Normdatensätze
 
	Goethe Faust kommt dann, das berühmte

Eule gesucht, kein Normdatum 532 Treffer
Mal ist eule im Titel, im Abstract oder  so drin aber ist ein Treffer
Jetzt welche Normdaten sind damit verknüpft?
Wenn Eule im Verlag, dann taucht der Verlag auf
  bietet Einstieg alles zu dieser Person
Hat Pseudonyme drin, weil unter anderem Namen publiziert
Alle diese Info stammen aus GND aber vieles auch in Marbach in Rahmen von Forschungsarbeiten ermittelt worden.
Wikipedia haben sie auch genommen und eingespielt


Normdatensätze können auch als Einstieg genommen werden für Suche
 
Unter Wegner= Nachlassbestände, das was er hinterlassen hat
Alles zu dieser Person dass wäre dann vergleichbar wie mit dem Suchschlitz.
 
Alles was er an Bilder dem Archiv vermacht hat suchen
 17  Kasten 
Totemmaske gehört noch dazu

Marbach hat einen gemeinsamen Index also Archiv, Bibliothek und Museum in einem gemeinsamen Index überführt.
 
 
Klick	Klick auf Lupe oder
 

Bestände sind nicht abgeschlossen, sie erschliessen jeden Tag
Datenprozessierung
Daten homogenisieren, vertikalen Suchmaschinen damit in Suchschema passen.
Muss daten vorbereiten, dass in Suchindex überführt werden
Dass muss täglich einmal durchlaufen werden für gesamtbestand
Dass dsind 3,6 Mio Daten täglich!!
 Das geht über Nacht
Unstrukturierte Textdateien mit Software namens Panda, werden in ein Tabellenformat
Aufbereitet, so dass  dann mit openrefine noch aus daten von wikipedia etc.
Alles wird aufbereitet z. b. Datumsbereiche für zeitliche Facette
Dann wird in TSV Format ähnlich CSV wieder in Solar Index eingespielt.
Einmal täglich!
Für Datenprozessierung muss so gemacht werden, dass eben Daten in zweite Index
Eingespielt werden, erst wenn alter Index gemacht wurde, wird umgeschaltet auf neuen Index. 
Sicherheit, dass ein Katalog sicher funktioniert.

**Alles war wir im Modul kennengelernt haben, haben sie in der Praxis auch angewendet am Beispiel von Marbach Katalog.**

Sonstiger Datenträger
Da gibt es CD Rom und Usb Stick aus versch. Leute
 
Über Bestände gehen
 


 
FRage: **Sind in dem Katalog auch elektronische Daten verzeichnet?
von Martin Heeb an alle:    9:27 AM
Also aus modernen Nachlässen?
Ja unter Beispiel Raddatz Fritz Hier gibt’s CD’s auch,	Fotokonvolut**

Datentankstelle ist geplant
 
 
**Anderes Projekt Schaufenster_projekt für open access***
 	
Einzelseite wird angezeigt hier kennt man aus Retrodigitalisate
Born digitales pdf sind in einer volltextsuche verzeichnet
Auch als pdf wird text herausgelesen!
 

	 
	Datentransformation: Über Schnittstellen wurden aus OAI-PMH Schnittstelle geladen und in Format Dublin Core herunterladen dann wurde in METS MOTS transformiert
 
	Diese Datenquelle wurde auf GitHub abgelegt.
 
	Sind auf dem Server so 20 Zeitschriften mit akutellen Ausgaben.
Ein ganz normaler Zeitschriftenserver da gibt es eine OAI-PMH Schnittstelle
Aufgabe von Lohmeier war es dann für jede Zeitschrift eine OAI
Dann eben Dublin Core wurde ein Mapping auf METS MOTS gemacht
 
Hier der Code auf git hub dazu
Hier die Regeln zum Konvertieren
 
 
	Diese werden dann auch wieder über eine OAI-PMH an Partnerfirma

 
	Git Hub Actions gehört zu Microsoft
Ganz ohne eigene Server zu betreiben, lassen sie jetzt diese prozesse täglich ablaufen
Haben git scrapping benutzt, also laden daten hinunter und speichern quch quelldaten Datensatz aus der Zeitschrift wird dann mit open refine konvertiert
Nachts alsowird gemacht, 500 stunden über git hub action sind gratis
 
Git Scrapping
 
Feuer in Kalifornien Daten kann man auch über git scrapping ganze historien anzeigen

Er sieht jetzt was geändert wurde hier wurde ein Abstract korrigiert
Inhaltliche änderungen kann man nachvollziehen.
 


**Datenreicherung mit open refine**
Habe ich gemacht die übung
Place of birth, date
11 Autoren suchen da…
Mark Baldwin ami Schriftsteller..
Reconillation machen
 

Bib Frame Modell (Nachfolger von Marc 21. Also dem Ableger Mods schlanke Form von Marc 21)
Für bibliothekarischen Datenaustasuch
War von Library of Congress, Washington
FRBR   und RDA basierend gemeinsames Regelwerk für biblio und Archiv
Werden aber von Bib Frame nicht zu 100% umgesetzt, gibt immer noch Sachen die nicht gehen
Wird Bib Frame erweitert
 
Datenmodell ist eben auf FRBR basiert ,unterscheidet von Werken und work 
Physisches Objekt. 3 Manifestationen.
Werk als Idee, nehmen wir Goethes Faust als Werk, dann gibt es Manifestation (Instance), z. b gedruckte Version. Dann gibt es noch das ITEM (Exemplare) von dem gedruckten Werk, wird ja nicht mehr als 1x gedruckt sonder 1000e. diese 3 Ebenen werden ganz abstract unterschieden, dasss ist das grosse Unterschied zu Marc 21. FRBR ist deutlich strikter.
•	Werk (Work)
•	Manifestation (Instance)
•	Exemplare (Item)

 
	Jede Entität hat ihren Datensatz und werden dann mit den Organisationen so verknüpft und mit Events (Geschreiben das Werke, überarbeitete Version des Werkes)
Dann gibt es auch Event-Entitäten. Diese werden mit dem Werk verknüpft.
Format: Ebook, pdf.. gedrucktes Buch..
Verlag: ist nicht mit Werk verknüpft sonder mit konkreter Ausgabe dieses Werk
Dann die Exemplarebene da kennt man schon von marc21 die Barcodes, Signatur dieses Werk zu finden ist. 
Man hat am Ende eine Art Baustruktur.


Wie ist die Beziehung von BIBFRAME zu FRBR? Dort sind ja vier Ebenen definiert. Bei BIBFRAME fehlt ja die Expression
von Lohmeier Felix an alle:    10:51 AM
Guter Punkt, Sebastian Meyer wird es gleich sicher noch kommentieren. Wikipedia sieht auch diesen Design-Unterschied: https://de.wikipedia.org/wiki/BIBFRAME#Design
von Lohmeier Felix an alle:    10:52 AM
BIBFRAME ist in RDF verfasst und basiert auf drei zentralen Abstraktionsebenen (Work, Instance, Item), mit drei zusätzlichen Klassen (Agent, Subject, Event) die mit den zentralen Kategorien verbunden sind.[7] Während sich Work in BIBFRAME als Zusammenführung der Kategorien Work und Expression[9] im FRBR-Modell der IFLA begreifen lässt, entspricht die Kategorie Instance in BIBFRAME der Kategorie Manifestation in FRBR. Hierbei handelt es sich um einen Bruch mit FRBR und den FRBR-basierten Resource Description and Access (RDA) Katalogisierungsregeln.
 
Marbach
Standards von Bib Frame und klassischem Format
Schauen sie bei Marbach noch keine Bib Frame
 Wenn Felder dann umbenennen, die Tabellen, wie bei Bib Frame heissen, dann ist es leichter ein Mapping auf BiB Frame Format zu machen.



 


	 

	 

	





 

Titel und Subclassen die diese erben die Eigenschaften der Oberklasse (Mutter)
Ontologie umfasst Beschreibungsklassen (Class), die spezifischen Eigenschaften(Property )verfügen.

Wenn nur ein Marc21 vor mir haben, solle ich alle Infos haben, alle sind aggregiert über Werk und Manifestation und all das zusammen in einem Datensatz. Bezeichnung für Personennamen, ohne weitere Quellen  kann so den Datensatz lesen. Der GND Identifier ist nicht drin.aber Marc sind halt Zahlen 205 A etc. wenn man nicht weiss, wass die Feldbezeichung heisst, dass ist in Bib Frame besser gelöst.
Bib Frame jede Entität wird alleine genommen. Man braucht vielzahl für die Gesamtbeschreibung.
Sehr  komplexe Beziehungskonflekt, aber man hat auch die Beziehungen drin, also so ein aUtoren name findet man in marc21 1000 Fach und bei Bib frame gibt’s genau einmal die Entität die diesen Autor beschreiben und diese ist dann einfach 1000 x verknüpft mit anderen Objekten (Werken, Manifestaitonen etc. dazu).

Wechsel auf Bib Frame noch nicht vollzogen, aber ist Zukunft.
Wie beim Telefon, je weiter umso schneller wird sich durchsetzten, dass zu adaptieren.

Alma was auf Marc 21 basiert, da kann man Person einmal anlegen und dann mit Datensätzen verknüpft. Gibt ja Oberfläche aber im Hintergrund, speichert zu jedem Datensatz die Person.
(Bei BibFrame wäre dass dann anderst, weil ja Beziehung und nur einmal ein Datensatz.)
Man muss also in Marbach beispielsweise der Autor dann 10'000 mal der Datensatz geändert werden.
Verbundzentrale in Deutschland, durch mehrfache Datenkonfigurationen, diese Marc21 Datensätze mit gleicher GND-Identifier dass dann nicht mehr 100% steht und bei BIB Frame wird es erzwungen durch das Format, darum bessere Qualitätskontrolle.
Man muss also Funktionen und Format, die Datensätze… also auch ein auf MARc21 basiertes Programm, kann gut sein.

**Ric (Records in Context)**
Das Bib Frame des Archivwesens
Basiert auch auf Linked Data Prinzipien.
Gibt eine schweizerische Projektgruppe, arbeitet eine Projektgruppe (Niklaus Stettler) mitbeteiligt.
Wir haben auch hier Events, Functions, Agents, Place, aber bei Bib Frame Bibliotheken ist einfacher strukturiet. Bei Bib Frame ist Werk im Hintergrund bei Ric ist Entstehungskonzept wichtig. Wie ist es entstanden.
Entwicklungsstand
0.2 veröffetnlicht
0 ist noch nicht stabil
Bibframe ist ja schon bei 2.0

 
 
 Atom ist ähnlich vielleicht dann wird wie alma für Ric
Atom ist schon gemacht
Gemäss Niklaus STettler, dass haben wir ja mal gesehen.
Man wird grosse Projekte haben
Erst in 5 Jahren
Erschliessungspraxis wird sich ändern stärker, weil Trend zu Normdaten (GND) da gibt’s auch dass die Archive diese dann verwenden.
Hochschularchiv der ETH nimmt auch GND anscheinend.






Wikidata hätte man auch mit open refine gearbeitet


**übung open refine**



Prinzip von open refine:

![Screenshot from 2022-01-13 16-46-28](https://user-images.githubusercontent.com/90834735/149363635-ba6c1ea9-91eb-4de8-8c33-968e8672a3a3.png)

auf dem TErminal wurde wieder das open refine geöffnet, es musst nicht mehr heruntergeladen werdenn, es war ja noch installiert von tag 5.

Hier die Befehle:
kuengjacquel@PCVDBAIN207:~$ wget https://github.com/OpenRefine/OpenRefine/releases/download/3.5.0/openrefine-linux-3.5.0.tar.gz
2022-01-13 10:14:08 (9.23 MB/s) - ‘openrefine-linux-3.5.0.tar.gz.1’ saved [131840764/131840764]

kuengjacquel@PCVDBAIN207:~$ cd ~/openrefine-3.5.0
kuengjacquel@PCVDBAIN207:~/openrefine-3.5.0$ ./refine


dann wurden die Daten (11 Autoren) (in Format csv) über die URL geladen.
Create Project > Web Addresses (URLs)
        https://raw.githubusercontent.com/libjohn/openrefine/master/data/AA-authors-you-should-read.csv
        Next
 das Projekt kann mit eine Namen abgespeichert werden.
    Create Project
  Die Ansicht wurde noch geändert,auf zeige 25, dass man alle 11 Autoren sehen kann.
![Screenshot from 2022-01-13 10-25-02](https://user-images.githubusercontent.com/90834735/149370289-ba27612d-5011-4b0f-a97c-c06b5da4baab.png)

Start Reconication:
![Screenshot from 2022-01-13 10-27-26](https://user-images.githubusercontent.com/90834735/149374578-4bc2f190-9bec-454b-884a-9a1e7ac15fff.png)
Man sollte Human (Mensch) anwählen als Property:
![Screenshot from 2022-01-13 10-28-15](https://user-images.githubusercontent.com/90834735/149374704-1930723c-f256-4b7f-a6a1-f3a11ca83e76.png)

dann kamen die Properties Mensch dazu:
![Screenshot from 2022-01-13 10-29-59](https://user-images.githubusercontent.com/90834735/149374821-12ebb26a-dc9c-4173-8f15-d14d168caf25.png)
James Baldwin Zelle 2 solle man nehmen, diese hatte schon die Wikidata Q... hinterlegt: (in Cell 2, James Baldwin, select the first option which a match of “(100)”)

![Screenshot from 2022-01-13 10-31-22](https://user-images.githubusercontent.com/90834735/149374982-54fd27e9-f3fe-496e-bd59-b3fcf5b39d0b.png)


In Cell 10, Click on the first name, then the second name. Do you see an African-American writer? Choose him by clicking the corresponding single check-mark
dann musste man diese zwei Zellen miteinander matchen.
![Screenshot from 2022-01-13 10-32-45](https://user-images.githubusercontent.com/90834735/149375807-92496ef2-e2de-41a6-a8ef-8dfaf195f17e.png)


dann kam das heraus.![Screenshot from 2022-01-13 10-34-50](https://user-images.githubusercontent.com/90834735/149375969-86cf9809-32f2-4d9d-aca6-713cc3194eb9.png)



die zweite Aufgabe war es Geburtstdatum und Todesdatum zu den Autoren zu ergänzen, das ging auch wieder über die Reconilation aber über die Values:
![Screenshot from 2022-01-13 10-40-11](https://user-images.githubusercontent.com/90834735/149376904-3456d4aa-5452-4042-9562-d2f08b014534.png)




![Screenshot from 2022-01-13 10-40-38](https://user-images.githubusercontent.com/90834735/149376165-9d04202b-74d5-49cd-93b1-49a28e2d5ffb.png)
dann habe ich es überprüft und es erschienen die zwei Daten:
![Screenshot from 2022-01-13 10-41-18](https://user-images.githubusercontent.com/90834735/149376229-10b9c62b-6a83-4909-8542-692ce64ae675.png)






**Wikidata    mit Suchanfrage ähnlich wie sparql**
 
96 Mio Datensätze auch elektronische Artikel  haben mit Mensch u. Erde angefangen.
Wikipedia ist ja das grösste Projekt von ihnen
Es soll aber nur einmal die Daten abglegt werden, in einer grossen strukturierten Datenbank.
Mensch war Q1
Erde Q2 
Dann eben Q1776290…hochgezählt siehe bei Carina Dahl
 
Wurde importiert aus englischen Wikipedia
 
Wikidata Anfrage
Ein spannende Zusammenfassung über die ANfrage habe ich hier gefunden:
![Screenshot from 2022-01-13 10-58-24](https://user-images.githubusercontent.com/90834735/149377108-fe119055-72b9-4c76-a4c0-09580dc454b4.png)


 

 
Viele Identifier
Viaf Identifiert 
ISNI Identifier
All Music Artist ID Identifier 
Facebook ID Identifier
YouTube Channel Identifier!

 
Carina Dahl ist a Instance of Mensch.
Oder Carina Dahl is property Female…

In Zukunft wird man noch mehr auf Wikidata setzen in Bibliothek und Archiv. In Marbach hat man ja Links auf Bilder dann genommen, man sollte aber Wikdata prüfen.
Aber Personenlebensdaten sollte man noch einen Plausabilitätscheck nehmen.
 








Urheberrechtsfreie Bilder gibt’s auch auf wikidata weil laufen unter Creative Common Lizenz
Das konnte auch in m arbach dann eingefügt werden, weil kein urheberschutz
 
 

	Wird viel mit Bots gearbeitet weil Datenautomatisiert
WIKIDATA mehr Daten nun als bei GND!
Ist zwar umstritten, weil jeder kann
Auch  wie bei wikipedia kann jeder mitmachen
Brockhause wurde eingestellt, wikipedia hat überholt. Obwohl nur brockhause nur experten schreiben
Aber wikidata kann GND dann auch überholen.















Sparql kann mit Tool Open Refine abfragen
Sparql super spannende Datenquelle auch für Zukunft.

Alex Stingson- soll man noch machen
Autoren von Nordafrika – selber Sparql -Abfrage schreiben
Aber man muss schaue


n ob in Wikidata drin sind.



Ich habe aus Spass die Katzen vom Englischen Premierminister gesucht, dazu nahm ich eine "Anfrage"-Vorlage, dass war die Vorlage:
![Screenshot from 2022-01-13 11-59-10](https://user-images.githubusercontent.com/90834735/149377798-52427538-48ba-4e2b-86bc-181f22e70d5d.png)
![Screenshot from 2022-01-13 18-20-57](https://user-images.githubusercontent.com/90834735/149378296-8c6936cf-ba29-47f0-b98e-fc58147499fa.png)

dabei bekam ich dann ein Ergebnis von Anzahl 153 mit allen KAtzen von höheren Rängen (Angestellten wie Premierministern etc.) und ihre hinterlegten Wikidata-Nummern. :
ich habe Peter und Toffe mal näher betrachtet und auf die Wikidata Nr. geklickt

![Screenshot from 2022-01-13 11-57-49](https://user-images.githubusercontent.com/90834735/149377406-3ef6bbfa-e44a-457f-a360-dc3a37219705.png)

Und ich habe noch eine andere Katze gefunden, mit Foto sogar auf mit einem eigenen Eintrag auf Wikidata, sie ist gut verlinkt und hat ihre eigenen Identifier.
![Screenshot from 2022-01-13 11-58-20](https://user-images.githubusercontent.com/90834735/149377438-82fe9e36-86ca-40fe-b3e1-7198c6ef2f47.png)

Von Sparql her, sagt mir jetzt dass "instance of Housecat", dass heisst sie ist eine Instanz (Untergruppe) von Hauskatze. Sie übernimmt also alles was schon bei Hauskatze drin ist.

 


Tagebücher
31. Januar  12 Uhr ist fertig!!
Abschlussartikel
Ältere Beiträge ergänzen Querverweise und Screenshots
In eigenen Worten schreiben!
 

	Halbes jahr laufen die virtuellen maschinen noch


    
    
    
    
    
    Diese Quellen sollte man sich einmal anschauen, ausser die Datenschule (diese ist nicht mehr so aktuell)
    
    Library Carpentry: https://librarycarpentry.org/lessons/
    Programming Historian: https://programminghistorian.org/en/lessons/
    openHPI: https://open.hpi.de/courses
    Datenschule: https://datenschule.de/lernmaterialien/
